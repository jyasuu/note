Here's a step-by-step guide to set up a CDC demo using PostgreSQL, Debezium, and Kafka:

### 1. Prerequisites
- Docker and Docker-compose installed
- Basic understanding of Kafka concepts
- curl, jq (for testing)

### 2. Setup Infrastructure with Docker

Create `docker-compose.yml`:
```yaml
version: '3.7'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092

  postgres:
    image: debezium/postgres:13
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - ./postgres-init.sql:/docker-entrypoint-initdb.d/postgres-init.sql

  connect:
    image: debezium/connect:1.9
    ports:
      - "8083:8083"
    depends_on:
      - kafka
      - postgres
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
```

### 3. Configure PostgreSQL for CDC
Create `postgres-init.sql`:
```sql
ALTER SYSTEM SET wal_level = logical;
CREATE USER debezium WITH REPLICATION LOGIN PASSWORD 'dbz';
CREATE DATABASE inventory;
GRANT ALL PRIVILEGES ON DATABASE inventory TO debezium;
```

### 4. Start Services
```bash
docker-compose up -d
```

### 5. Create Debezium Connector
```bash
curl -i -X POST -H "Accept:application/json" \
  -H "Content-Type:application/json" \
  localhost:8083/connectors/ \
  -d '{
    "name": "inventory-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "debezium",
      "database.password": "dbz",
      "database.dbname": "inventory",
      "database.server.name": "dbserver1",
      "table.include.list": "public.*",
      "plugin.name": "pgoutput"
    }
  }'
```

### 6. Test the Setup

**Create test table and data:**
```bash
docker-compose exec postgres psql -U postgres -d inventory

-- In PSQL console:
CREATE TABLE customers (
  id SERIAL PRIMARY KEY,
  name VARCHAR(255) NOT NULL,
  email VARCHAR(255) NOT NULL
);

INSERT INTO customers (name, email) VALUES
('John Doe', 'john@example.com'),
('Jane Smith', 'jane@example.com');
```

**Check Kafka topics:**
```bash
docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

**Consume messages from Kafka:**
```bash
docker-compose exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic dbserver1.public.customers \
  --from-beginning
```

### 7. Test CDC Operations

Make changes in PostgreSQL and watch Kafka messages:
```sql
UPDATE customers SET email='john.doe@example.com' WHERE id=1;
DELETE FROM customers WHERE id=2;
```

### 8. Clean Up
```bash
docker-compose down -v
```

### Key Components Explained:
1. **PostgreSQL**: Source database configured for logical replication
2. **Debezium**: Captures changes from PostgreSQL WAL (Write-Ahead Log)
3. **Kafka Connect**: Manages connectors and streams data to Kafka topics
4. **Kafka**: Distributed event streaming platform
5. **Zookeeper**: Coordination service for Kafka

### Troubleshooting Tips:
1. Check connector status:
   ```bash
   curl -s localhost:8083/connectors/inventory-connector/status | jq
   ```
2. View connector logs:
   ```bash
   docker-compose logs connect
   ```
3. Verify PostgreSQL replication settings:
   ```bash
   docker-compose exec postgres cat /var/lib/postgresql/data/postgresql.conf | grep wal_level
   ```

This POC demonstrates:
- Real-time data capture from PostgreSQL
- Change event streaming to Kafka
- Support for insert/update/delete operations
- Schema evolution tracking

For production use, you'd want to add:
- Security (SSL, authentication)
- Monitoring
- Error handling
- Schema registry
- Proper retention policies
- Horizontal scaling


Certainly! Let's go through a step-by-step practice guide for PostgreSQL's `pgoutput` logical decoding plugin. This plugin is used for logical replication and streaming changes from your database.

---

### **1. Configure PostgreSQL for Logical Replication**
First, ensure your PostgreSQL server is configured to support logical replication.

#### **Update `postgresql.conf`:**
```ini
wal_level = logical             # Enable logical WAL logging
max_wal_senders = 10           # Number of processes for WAL sending
max_replication_slots = 10     # Number of replication slots
```
Restart PostgreSQL to apply changes:
```bash
sudo systemctl restart postgresql
```

---

### **2. Create a Replication User**
Create a dedicated user with replication privileges:
```sql
CREATE ROLE repl_user WITH LOGIN REPLICATION PASSWORD 'secure_password';
```

---

### **3. Create a Publication**
Define which tables to replicate. Here, weâ€™ll use a sample table `users`:
```sql
-- Create a test table
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    email TEXT UNIQUE
);

-- Create a publication for the table
CREATE PUBLICATION my_pub FOR TABLE users;
```

---

### **4. Create a Replication Slot**
A replication slot ensures WAL logs are retained until theyâ€™re consumed. Use `pgoutput` as the plugin:
```sql
SELECT pg_create_logical_replication_slot(
    'my_slot', 
    'pgoutput'
);
```

Verify the slot exists:
```sql
SELECT * FROM pg_replication_slots;
```

---

### **5. Stream Changes Using `pg_recvlogical`**
Use the `pg_recvlogical` command-line tool to stream changes from the slot. Open a terminal and run:
```bash
pg_recvlogical \
  -d your_database_name \
  --slot my_slot \
  --start \
  -f - \
  -o pretty-print=1 \
  -o add-tables=public.users \
  -U repl_user \
  -P secure_password
```

---

### **6. Test Replication**
Insert/update data in the `users` table and observe the changes in the `pg_recvlogical` output.

#### **Example:**
```sql
INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com');
UPDATE users SET email = 'alice_new@example.com' WHERE id = 1;
DELETE FROM users WHERE id = 1;
```

---

### **7. Decoding the Output**
`pgoutput` outputs data in binary format. To interpret it, youâ€™ll typically use a client library (e.g., in Python, Java). Hereâ€™s a basic Python example using `psycopg2`:

```python
import psycopg2
from psycopg2 import sql

conn = psycopg2.connect(
    dbname="your_db",
    user="repl_user",
    password="secure_password",
    host="localhost"
)

conn.set_session(autocommit=True)
cur = conn.cursor()

# Poll for changes
cur.execute("SELECT data FROM pg_logical_slot_get_binary_changes('my_slot', NULL, NULL);")
changes = cur.fetchall()

for change in changes:
    print(change[0].tobytes())  # Raw binary output (requires parsing)
```

---

### **8. Create a Subscription (Optional)**
To replicate to another PostgreSQL database, create a subscription on the subscriber node:
```sql
-- On the subscriber database:
CREATE SUBSCRIPTION my_sub
CONNECTION 'host=localhost dbname=your_db user=repl_user password=secure_password'
PUBLICATION my_pub
WITH (copy_data = true);
```

---

### **Key Notes:**
- **Replica Identity:** For `UPDATE`/`DELETE` to work, ensure the table has a `REPLICA IDENTITY` (usually the primary key).
  ```sql
  ALTER TABLE users REPLICA IDENTITY USING INDEX users_pkey;
  ```
- **Permissions:** The replication user needs `SELECT` privileges on the published tables.
- **Binary Format:** `pgoutput` uses a binary format. Use tools like Debezium or write a decoder for structured data.

---

### **Troubleshooting**
- **No changes visible?** Ensure the publication includes the correct tables.
- **Permissions error?** Grant `REPLICATION` and `SELECT` privileges to `repl_user`.
- **Slot not advancing?** Always acknowledge changes by consuming them (e.g., via `pg_recvlogical` or a subscription).

---

Let me know if youâ€™d like to dive deeper into any specific step! ðŸš€
