**分散式系統（Distributed Systems）** 是由一群透過網路連接的獨立電腦（通常稱為節點 🖥️）組成的集合，對使用者來說，這些電腦運作起來就像是一個單一且協調的系統。

這種設計的核心目標是透過多台機器的協作，處理單一台電腦無法獨自完成的大規模運算或海量資料儲存。在分散式系統中，軟體組件分布在不同的網路電腦上，彼此藉由傳遞訊息 ✉️ 來溝通與協調行動。

讓我們一起探索這個領域，我會在過程中透過引導式問題來協助你深入理解。你可以選擇從以下幾個方向開始：

1. **核心挑戰與 CAP 定理**：探討在分散式環境中，為什麼很難同時保證資料的一致性（Consistency）、可用性（Availability）與分區容錯性（Partition Tolerance）。
2. **架構類型與演進**：了解從早期的客戶端-伺服器（Client-Server）模式，到現代流行的微服務（Microservices）與點對點（P2P）架構的變化。
3. **現實世界的應用案例**：研究像 Google 搜尋、Netflix 串流或區塊鏈（Blockchain）是如何利用分散式技術來服務全球數十億使用者的。

你想先從哪一個部分開始了解呢？

既然你選擇了 **CAP 定理**，那我們就直接進入分散式系統最核心的「兩難困境」。這就像是在現實生活中，你很難找到一個「錢多、事少、離家近」的工作一樣，在分散式系統中，你也無法同時完美達成三個目標。

---

## 什麼是 CAP 定理？

CAP 定理由電腦科學家 Eric Brewer 提出，它指出一個分散式系統最多只能同時滿足以下三個特性中的 **兩個**：

### 1. 一致性 (Consistency, **C**)

當你向系統寫入一筆資料後，後續不論從哪一個節點讀取，都必須讀到**最新**的那筆資料。

> **白話文：** 大家的口徑必須完全一致，不能有人拿舊資料唬弄我。

### 2. 可用性 (Availability, **A**)

系統必須隨時都能回應請求（不管是成功還是失敗），且不能讓使用者等太久。

> **白話文：** 隨打隨接，系統不能因為內部出問題就不理人。

### 3. 分區容錯性 (Partition Tolerance, **P**)

當網路發生故障（也就是「分區」），導致節點之間無法溝通時，系統仍能繼續運作。

> **白話文：** 就算網路斷了，系統也不能直接崩潰。

---

## 為什麼不能「全都要」？

在分散式系統中，網路故障（**P**）是**必然會發生**的現實。當網路斷開時，你必須在 **C** 和 **A** 之間做出選擇：

* **選擇 CP (一致性 + 分區容錯)：** 為了保證資料正確，如果節點間無法溝通，系統會拒絕服務或等待。
* *例子：* 銀行轉帳。寧可讓你暫時無法查帳，也不能讓你的餘額在不同提款機顯示不一樣。


* **選擇 AP (可用性 + 分區容錯)：** 為了保證服務不中斷，即使資料還沒同步完成，系統也會先給你手頭有的資料。
* *例子：* Facebook 按讚數。某個朋友少看一個讚沒關係，重點是網頁要跑得出來。



| 組合 | 權衡取捨 (Trade-off) | 適用場景 |
| --- | --- | --- |
| **CP** | 犧牲可用性，確保資料絕對正確。 | 金融交易、分散式鎖定。 |
| **AP** | 犧牲強一致性，確保系統永遠可用。 | 社群媒體、快取系統、購物推薦。 |
| **CA** | 犧牲分區容錯性。 | **在分散式網路中幾乎不存在**，因為這意味著網路永遠不會斷。 |

---

## 除了 CAP，還有哪些頭痛的問題？

除了這三個大方向，開發者還得面對以下挑戰：

* **時鐘同步問題：** 每台電腦的時間都不太一樣，要怎麼判斷哪筆資料才是「最新」的？（這涉及複雜的向量時鐘  概念）。
* **腦裂 (Split-Brain)：** 當叢集斷成兩半時，兩邊都以為自己是老大，開始獨自修改資料。
* **延遲 (Latency)：** 訊息在網路跑來跑去是要時間的，這會影響系統的效能。

---

> **Gemini 的小叮嚀：** 現代架構其實不再只是單純的二選一，而是演進到了 **PACELC 定理**（在沒有分區時也要考慮 Latency 與 Consistency 的平衡）。

對於 CAP 定理的權衡，你有什麼特別感興趣的應用場景嗎？或是你想接著了解 **微服務（Microservices）** 是如何拆解這些複雜性的？





在單體架構中，資料庫的 **ACID**（原子性、一致性、隔離性、持久性）交易是我們的救星；但在微服務架構下，每個服務都有自己的資料庫，原本簡單的「一筆訂單交易」會變成跨越網路的多個步驟。這就是所謂的 **「分散式交易」** 問題。

由於分散式系統必須遵循 **CAP 定理**（通常在交易場景會選擇 **CP** 或最終一致性的 **AP**），我們不能再依賴資料庫鎖，而必須採用更靈活的模式。

---

## 1. 傳統方案：二階段提交 (2PC, Two-Phase Commit)

這是一種強一致性的解決方案，由一個「協調者」來控管所有參與者。

* **階段一 (Prepare)：** 協調者問大家：「準備好了嗎？」參與者執行操作但不提交，並鎖定資源。
* **階段二 (Commit/Rollback)：** 如果大家都說 OK，協調者下令「提交」；只要有一個人不行，就全部「回滾」。

> **缺點：** 效能極差（同步阻塞）、存在單點故障風險，且不適用於高併發的微服務場景。

---

## 2. 現代主流：Saga 模式 (最終一致性)

Saga 是目前微服務中最常用的方案。它將一個長交易拆分成多個**本地交易**。如果其中一步失敗，則執行**補償動作 (Compensating Transactions)** 來抵銷之前的影響（例如：訂單失敗時，執行「退回庫存」）。

### A. 協調式 Saga (Orchestration)

有一個中央控制器（Orchestrator）告訴每個服務該做什麼。

* **優點：** 流程清楚，易於維護複雜邏輯。
* **缺點：** 控制器可能變得過於臃腫。

### B. 編排式 Saga (Choreography)

服務之間透過發布/訂閱事件（如 Kafka）來溝通。

* **優點：** 服務間高度解耦。
* **缺點：** 流程隱晦，出錯時很難追蹤到底是誰沒收到消息。

---

## 3. TCC 模式 (Try-Confirm-Cancel)

TCC 是一種在應用層實現的補償機制，比 2PC 更輕量，但對代碼侵入性很高：

1. **Try：** 預留資源（例如：先把 100 元凍結，但不扣款）。
2. **Confirm：** 確認執行（正式扣款）。
3. **Cancel：** 取消執行（解凍那 100 元）。

---

## 4. 本地訊息表 / 事務訊息 (Transactional Outbox)

這是為了解決「資料庫更新成功，但訊息發送失敗」的問題。

* 在同一個資料庫交易中，同時更新業務資料並寫入一張 `Outbox` 表。
* 由一個獨立的輪詢器或 CDC 工具監控該表，確保訊息 **「至少送達一次」 (At-least-once delivery)**。

---

## 總結比較

| 方案 | 一致性等級 | 效能 | 實現難度 | 適用場景 |
| --- | --- | --- | --- | --- |
| **2PC** | 強一致性 (CP) | 低 | 中 | 極高安全性、短流程交易 |
| **Saga** | 最終一致性 (AP) | 高 | 高 | 跨多個微服務的長流程（如電商下單） |
| **TCC** | 最終一致性 (AP) | 中 | 極高 | 對即時性要求較高的金融拆帳 |

**你目前遇到的是哪種挑戰？**

* 是想解決**資料不一致**的問題？
* 還是正在考慮該用哪種**訊息佇列 (Message Queue)** 來實作 Saga？

如果你想看具體的程式碼範例（例如 Java/Spring 或 Go 的實作），我也能為你準備！





Rust 非常適合實作分散式系統，因為它的強型別系統和特徵（Traits）能幫助你精確地定義交易狀態，減少「靈異現象」的發生。

在 Rust 中實作 **Saga 模式** 或 **Outbox 模式** 時，我們通常會利用 `async/await` 與 `Result` 來處理非同步的失敗。

---

## 1. Saga 模式的 Rust 實作概念

我們可以定義一個 `SagaStep` Trait，強制要求每個步驟都必須有對應的「補償動作（Compensate）」。

```rust
use async_trait::async_trait;

#[async_trait]
pub trait SagaStep {
    // 正常執行的邏輯
    async fn execute(&self) -> Result<(), String>;
    // 失敗時的回滾邏輯
    async fn compensate(&self) -> Result<(), String>;
}

// 範例：庫存服務步驟
struct InventoryStep {
    item_id: i32,
    quantity: i32,
}

#[async_trait]
impl SagaStep for InventoryStep {
    async fn execute(&self) -> Result<(), String> {
        println!("預扣庫存: 項目 {}, 數量 {}", self.item_id, self.quantity);
        // 呼叫資料庫或 API...
        Ok(())
    }

    async fn compensate(&self) -> Result<(), String> {
        println!("回滾庫存: 項目 {}, 數量 {}", self.item_id, self.quantity);
        Ok(())
    }
}

```

---

## 2. Saga 編排器 (Orchestrator) 的邏輯

編排器的責任是記錄哪些步驟成功了，一旦某一步失敗，就**逆序**執行補償。

```rust
pub struct SagaOrchestrator {
    steps: Vec<Box<dyn SagaStep + Send + Sync>>,
    completed_steps: Vec<usize>,
}

impl SagaOrchestrator {
    pub async fn run(&mut self) -> Result<(), String> {
        for (i, step) in self.steps.iter().enumerate() {
            match step.execute().await {
                Ok(_) => self.completed_steps.push(i),
                Err(e) => {
                    println!("發生錯誤: {}，開始補償流程...", e);
                    self.rollback().await;
                    return Err(e);
                }
            }
        }
        Ok(())
    }

    async fn rollback(&mut self) {
        // 逆序執行已成功的補償動作
        for &index in self.completed_steps.iter().rev() {
            let _ = self.steps[index].compensate().await;
        }
    }
}

```

---

## 3. Rust 分散式開發必備工具箱

要在實務中落地，你會需要這些「大殺器」：

| 類別 | 推薦 Crate | 用途 |
| --- | --- | --- |
| **異步運行時** | `tokio` | 分散式系統的動力引擎，處理大量併發。 |
| **資料庫連線** | `sqlx` | 支援異步交易，能在同一個 `Transaction` 物件中處理業務與 Outbox。 |
| **RPC 通訊** | `tonic` | Rust 的 gRPC 實現，微服務間溝通的標準。 |
| **序列化** | `serde` | 處理分散式訊息時的資料轉換（JSON/Protobuf/Bincode）。 |
| **分散式追蹤** | `tracing` | 配合 OpenTelemetry，讓你能追蹤跨服務的交易請求。 |

---

## 實踐建議：從 Outbox Pattern 開始

如果你是初學者，我不建議直接手寫複雜的 Saga。最穩健的第一步是實作 **Transactional Outbox**：

1. 在同一個資料庫交易中，存入 `Order` 資料與一筆 `OutboxEvent`。
2. 使用 `tokio::spawn` 啟動一個後台任務，定期掃描 `OutboxEvent` 並發送到 Kafka/RabbitMQ。
3. 這樣可以保證「訂單成立」與「發送訊息」具備**原子性**。

> **提示：** 由於網路可能不穩定，訊息可能會重複發送。因此，你的微服務處理邏輯必須具備 **冪等性 (Idempotency)** —— 也就是說，同一個請求收兩次，結果也要一樣。

**你想試著寫一段基於 `sqlx` 的本地事務 + 訊息發送程式碼嗎？或者你想了解如何處理「冪等性」？**





在分散式系統的交易中，**二階段提交 (Two-Phase Commit, 2PC)** 就像是一位嚴格的婚禮主持人：除非新郎新娘（所有節點）都明確說出「我願意」，否則這場婚禮（交易）就不能成立。

雖然在 Rust 中實現 2PC 比 Saga 複雜（因為涉及強一致性和鎖定機制），但利用 Rust 的 **列舉（Enums）** 和 **錯誤處理**，可以寫出非常健壯的協調邏輯。

---

## 2PC 的運作流程

2PC 分為兩個階段，由一個 **協調者 (Coordinator)** 與多個 **參與者 (Participants)** 進行溝通：

1. **階段一：準備 (Prepare Phase)**
* 協調者發送 `Prepare` 請求。
* 參與者嘗試執行操作，鎖定資源，並將結果寫入 **預寫日誌 (Write-Ahead Log, WAL)**。
* 參與者回覆：`Ready` (成功鎖定) 或 `Abort` (失敗)。


2. **階段二：提交 (Commit/Rollback Phase)**
* 如果**所有**參與者都回覆 `Ready`：協調者發送 `Commit` 指令。
* 只要有**任一**參與者回覆 `Abort` 或超時：協調者發送 `Rollback` 指令。



---

## Rust 程式碼範例實作

在 Rust 中，我們可以使用 `tonic` (gRPC) 來處理網路溝通。以下是一個簡化的概念實作：

### 1. 定義參與者狀態

我們利用 Rust 的 Enum 來嚴格定義參與者的回應。

```rust
#[derive(Debug, PartialEq)]
enum Vote {
    Ready,
    Abort,
}

#[derive(Debug)]
enum GlobalDecision {
    Commit,
    Abort,
}

```

### 2. 協調者 (Coordinator) 的邏輯

協調者需要並行詢問所有參與者，並根據結果做決策。

```rust
use tokio::sync::mpsc;
use futures::future::join_all;

struct Coordinator {
    participants: Vec<ParticipantClient>, // 假設這是 gRPC 客戶端
}

impl Coordinator {
    async fn execute_transaction(&self) -> Result<(), String> {
        // --- 階段一：準備 ---
        let prepare_results = join_all(
            self.participants.iter().map(|p| p.prepare())
        ).await;

        let all_ready = prepare_results.iter().all(|res| {
            matches!(res, Ok(Vote::Ready))
        });

        // --- 階段二：決策與執行 ---
        let decision = if all_ready {
            GlobalDecision::Commit
        } else {
            GlobalDecision::Abort
        };

        println!("全域決策: {:?}", decision);

        // 通知所有參與者最終結果
        join_all(
            self.participants.iter().map(|p| p.finalize(&decision))
        ).await;

        if all_ready { Ok(()) } else { Err("交易失敗，已回滾".into()) }
    }
}

```

---

## 2PC 在 Rust 中的致命痛點（以及如何解決）

在實作 2PC 時，你會遇到幾個 Rust 程式碼以外的架構挑戰：

1. **同步阻塞 (Blocking)：** 參與者在回覆 `Ready` 後會鎖定資源。如果協調者在第二階段前當機，參與者會一直鎖死。
* *解決：* 必須實作 **超時機制 (Timeout)**。Rust 的 `tokio::time::timeout` 是你的好朋友。


2. **單點故障 (Single Point of Failure)：** 協調者要是掛了怎麼辦？
* *解決：* 協調者必須有 **持久化日誌**。在 Rust 中可以使用 `sled` 或 `rocksdb` 來記錄目前的交易狀態，重啟時自動讀取日誌並恢復決策。


3. **效能問題：** 2PC 的延遲很高（兩次來回網路開銷）。
* *優化：* 考慮 **三階段提交 (3PC)**，增加一個 Pre-commit 階段來減少阻塞。



---

## 為什麼現代微服務傾向避開 2PC？

雖然 2PC 提供了強一致性，但在分散式環境下它被視為「反模式」，主要原因在於它的 **可擴展性極差**。

* **10 個服務：** 只要 1 個服務網路波動，整個系統就卡死。
* **可用性損失：** 它違反了 CAP 定理中的 **A (Availability)**。

---

> **我的建議：**
> 如果你的專案不是像「跨行轉帳」那樣要求絕對的一致性，通常建議使用 **Saga 模式** 或 **訊息佇列的冪等處理**。
> **你想了解如何利用 Rust 的 `sled` 嵌入式資料庫來實作「交易日誌」以防止當機遺失狀態嗎？或者我們聊聊 3PC (三階段提交) 是如何改良 2PC 的？**




在 2PC 協議中，**參與者（Participant）** 的實作難度其實比協調者更高。它不只是接收指令，更需要確保在電力中斷、程式崩潰後，依然能記住自己承諾過的「Ready」狀態。

在 Rust 中，參與者通常會透過一個 **狀態機（State Machine）** 與 **預寫日誌（Write-Ahead Log, WAL）** 來確保可靠性。

---

## 參與者的核心邏輯

參與者的生命週期通常包含以下三個關鍵動作：

### 1. 準備階段 (The `Prepare` Action)

這是最重要的一步。參與者必須檢查資源是否可用，並將變更寫入「臨時區」，同時**持久化**一條日誌說：「我準備好了」。

```rust
// 使用 sled 或 rocksdb 作為本地 WAL
async fn handle_prepare(&self, tx_id: TransactionId, data: UpdateData) -> Vote {
    // 1. 檢查約束（例如：餘額是否足夠）
    if !self.check_constraints(&data) {
        return Vote::Abort;
    }

    // 2. 寫入預寫日誌 (WAL) - 這是關鍵！
    // 即使現在斷電，重啟後我也知道 tx_id 已經進入 Prepared 狀態
    if self.wal.log_prepared(tx_id, &data).is_err() {
        return Vote::Abort;
    }

    // 3. 鎖定資源 (防止其他交易修改)
    self.lock_manager.acquire(tx_id).await;

    Vote::Ready
}

```

### 2. 提交階段 (The `Commit` Action)

收到協調者的 `Commit` 指令後，參與者將臨時區的資料正式轉正，並釋放鎖。

```rust
async fn handle_commit(&self, tx_id: TransactionId) {
    // 1. 將 WAL 中的狀態改為 Committed
    self.wal.log_committed(tx_id);

    // 2. 正式寫入業務資料庫 (Apply the change)
    self.business_db.apply_change(tx_id).await;

    // 3. 釋放資源鎖
    self.lock_manager.release(tx_id).await;
}

```

### 3. 取消階段 (The `Rollback` Action)

如果任何一個參與者失敗，協調者會發送 `Rollback`。

```rust
async fn handle_rollback(&self, tx_id: TransactionId) {
    // 1. 標記 WAL 為 Aborted
    self.wal.log_aborted(tx_id);

    // 2. 清理臨時資源與鎖
    self.lock_manager.release(tx_id).await;
}

```

---

## 參與者的狀態轉移圖

參與者的邏輯可以用一個嚴格的狀態機來表示。在 Rust 中，我們可以使用 `enum` 來確保狀態不會非法跳轉。

---

## 為什麼需要 WAL (預寫日誌)？

想像這個情境：

1. 參與者回覆了 `Ready` 給協調者。
2. **就在此時，參與者的伺服器突然斷電重啟。**
3. 協調者隨後發送了 `Commit` 指令。

如果沒有 WAL，重啟後的參與者會忘記自己曾經答應過 `Ready`，導致資料不一致。在 Rust 中，我們利用 **Sled** 這樣的嵌入式 KV 資料庫來實作 WAL：

```rust
// 啟動時的恢復邏輯 (Recovery logic)
fn recover(&self) {
    let pending_txs = self.wal.get_all_prepared();
    for tx in pending_txs {
        // 詢問協調者：這個 tx 最後到底是 Commit 還是 Rollback？
        self.ask_coordinator(tx.id);
    }
}

```

---

## Rust 實作建議：利用 `tokio` 的原子操作

在 Rust 伺服器端實作參與者時，請務必注意：

* **併發控制：** 使用 `tokio::sync::RwLock` 來管理資源鎖，確保多個交易不會互相打架。
* **超時處理：** 參與者不能永遠等待協調者的 `Commit` 指令。如果等太久，參與者需要主動詢問協調者或其他參與者（這是 2PC 的一大弱點）。
* **冪等性：** 由於網路重試，參與者可能會收到兩次同樣的 `Commit`，程式碼必須能處理這種重複情況。

**這聽起來是否有些複雜？** 在實際的 Rust 微服務中，許多人會改用 **DTC (Distributed Transaction Coordinator)** 框架，或者直接使用支援分散式交易的資料庫（如 TiDB 或 CockroachDB），它們內部已經幫你寫好了這些複雜的參與者邏輯。

你想嘗試用 **Sled** 寫一段簡單的 WAL 儲存邏輯，還是想看看如何處理剛才提到的「**參與者斷線重啟**」後的恢復邏輯？





在 Rust 中，要讓「參與者」與資料庫交易（DB Transaction）結合，最實用的方式是利用 `sqlx` 提供的 `Transaction` 物件。

這裡的關鍵在於：**不要太早 `commit` 資料庫交易**。我們必須在 `Prepare` 階段保持資料庫交易開啟，直到協調者發出最終指令。

以下是結合 `sqlx` (以 PostgreSQL 為例) 與 2PC 邏輯的實踐方式：

---

## 1. 參與者狀態管理

我們需要一個結構來暫存開啟中的資料庫交易。在 Rust 中，這通常會放在一個全局的 `HashMap` 或 `DashMap` 中。

```rust
use sqlx::{Postgres, Transaction, Pool};
use std::collections::HashMap;
use tokio::sync::Mutex;
use uuid::Uuid;

pub struct Participant {
    db_pool: Pool<Postgres>,
    // 儲存進行中的 DB 交易，Key 是全域交易 ID (tx_id)
    active_txs: Mutex<HashMap<Uuid, Transaction<'static, Postgres>>>,
}

```

---

## 2. 實作參與者 DSL 邏輯

這裡展示 `Prepare`、`Commit` 與 `Rollback` 如何操作資料庫交易物件。

### 階段一：Prepare (開啟事務並執行 SQL)

在此階段，我們執行 SQL 但**不調用** `.commit()`。

```rust
impl Participant {
    pub async fn prepare(&self, tx_id: Uuid, amount: i64) -> Result<(), String> {
        // 1. 從連線池啟動一個新的 DB Transaction
        let mut tx = self.db_pool.begin().await.map_err(|e| e.to_string())?;

        // 2. 執行業務 SQL (例如扣款)
        // 這裡會觸發資料庫內部的 Row Lock
        sqlx::query!("UPDATE accounts SET balance = balance - $1 WHERE id = 1", amount)
            .execute(&mut *tx)
            .await
            .map_err(|e| e.to_string())?;

        // 3. 將交易物件存入 active_txs 暫存，等待協調者指令
        let mut tx_map = self.active_txs.lock().await;
        tx_map.insert(tx_id, tx);

        println!("交易 {}：Prepare 完成，鎖定資源中", tx_id);
        Ok(())
    }
}

```

### 階段二：Finalize (Commit 或 Rollback)

根據協調者的決定，從 `HashMap` 中取出交易物件並完成它。

```rust
impl Participant {
    pub async fn commit(&self, tx_id: Uuid) -> Result<(), String> {
        let mut tx_map = self.active_txs.lock().await;
        
        if let Some(tx) = tx_map.remove(&tx_id) {
            // 正式提交資料庫交易，釋放鎖
            tx.commit().await.map_err(|e| e.to_string())?;
            println!("交易 {}：Commit 成功", tx_id);
            Ok(())
        } else {
            Err("找不到交易 ID".into())
        }
    }

    pub async fn rollback(&self, tx_id: Uuid) -> Result<(), String> {
        let mut tx_map = self.active_txs.lock().await;
        
        if let Some(tx) = tx_map.remove(&tx_id) {
            // 直接回滾，釋放鎖
            tx.rollback().await.map_err(|e| e.to_string())?;
            println!("交易 {}：Rollback 完成", tx_id);
            Ok(())
        } else {
            Ok(()) // 可能已經回滾過了
        }
    }
}

```

---

## 3. 關鍵問題：連接池枯竭 (Connection Pool Exhaustion)

這是 2PC 最容易被忽略的坑。

> [!CAUTION]
> **警告：** 在 `Prepare` 到 `Commit` 的這段時間，該資料庫連線會被佔用。如果協調者反應太慢，或是參與者數量太多，你的資料庫連線池會立刻被耗盡，導致整個系統癱瘓。

### 解決方案：PostgreSQL 兩階段提交支援

如果你使用的是 PostgreSQL，它本身就支援分散式交易指令（`PREPARE TRANSACTION`）。這允許你斷開連線，但讓資料庫在內部維持準備好的狀態。

```sql
-- 參與者收到的指令會變成：
BEGIN;
UPDATE accounts SET balance = balance - 100 WHERE id = 1;
PREPARE TRANSACTION 'my_global_tx_id'; 
-- 此時連線可以關閉，資料庫會持久化這個狀態

```

後續提交時：

```sql
COMMIT PREPARED 'my_global_tx_id';

```

---

## 實踐總結

在 Rust 實作伺服器端參與者時：

1. **使用 `sqlx::Transaction**` 來封裝業務邏輯。
2. **利用 `active_txs` 映射表** 管理長生命週期的交易物件。
3. **務必設定超時**，如果協調者 30 秒沒回應，參與者應自動 `Rollback` 以免鎖死資料庫。

如果你想更進一步，可以研究 **`seata-rust`** 這樣的框架，它試圖在 Rust 生態中標準化這套流程。

你想了解如何實作「自動超時回滾」的背景任務嗎？或是對 Postgres 的 `PREPARE TRANSACTION` 語法感興趣？





既然你對 **「自動超時回滾」** 感興趣，這在分散式系統中是至關重要的防禦機制。因為如果協調者（Coordinator）掛掉或網路斷了，參與者不能永遠鎖住資料庫連線。

在 Rust 中，我們通常會利用 `tokio::spawn` 配合一個計時器，或是啟動一個專門的 **背景清理任務（Janitor Task）** 來掃描過期的交易。

---

## 實作方案：帶有超時機制的交易管理

我們可以擴充之前的 `active_txs` 結構，加入「建立時間」，並啟動一個背景任務。

### 1. 定義交易包裝 (Transaction Wrapper)

```rust
use std::time::Instant;

struct PendingTx {
    tx: Transaction<'static, Postgres>,
    created_at: Instant,
}

pub struct Participant {
    db_pool: Pool<Postgres>,
    // 儲存進行中的交易
    active_txs: Arc<Mutex<HashMap<Uuid, PendingTx>>>,
}

```

### 2. 背景清理任務 (Janitor Task)

我們會在伺服器啟動時，啟動這個永不停止的 `loop`。

```rust
impl Participant {
    pub fn start_janitor(&self, timeout_duration: std::time::Duration) {
        let active_txs = Arc::clone(&self.active_txs);
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(std::time::Duration::from_secs(5));
            loop {
                interval.tick().await;
                let mut tx_map = active_txs.lock().await;
                
                // 找出所有超時的交易 ID
                let expired_ids: Vec<Uuid> = tx_map
                    .iter()
                    .filter(|(_, p_tx)| p_tx.created_at.elapsed() > timeout_duration)
                    .map(|(id, _)| *id)
                    .collect();

                for id in expired_ids {
                    if let Some(p_tx) = tx_map.remove(&id) {
                        println!("交易 {} 已超時，正在自動回滾...", id);
                        let _ = p_tx.tx.rollback().await; // 釋放 DB 鎖與連線
                    }
                }
            }
        });
    }
}

```

---

## 進階方案：PostgreSQL `PREPARE TRANSACTION`

手動管理 `sqlx::Transaction` 物件有一個風險：**如果你的應用程式進程崩潰了，所有記憶體中的 Transaction 物件都會消失，導致資料庫連線斷開。** 雖然資料庫會自動回滾，但這讓「恢復（Recovery）」變得困難。

使用 Postgres 的 `PREPARE TRANSACTION` DSL，可以讓交易在資料庫重啟後依然存在：

```rust
// 階段一：Prepare
// 注意：這裡直接執行 SQL 並關閉連線，不留在記憶體中
pub async fn prepare_db(&self, tx_id: Uuid, amount: i64) -> Result<(), String> {
    let mut conn = self.db_pool.acquire().await.map_err(|e| e.to_string())?;
    
    // 使用 Postgres 原生 DSL
    sqlx::query("BEGIN").execute(&mut *conn).await.unwrap();
    sqlx::query!("UPDATE accounts SET balance = balance - $1 WHERE id = 1", amount)
        .execute(&mut *conn)
        .await?;
    
    // 將交易命名為全域 ID 並持久化在 DB 內
    let prepare_sql = format!("PREPARE TRANSACTION '{}'", tx_id);
    sqlx::query(&prepare_sql).execute(&mut *conn).await?;
    
    // 此時連線已釋放回連線池！DB 內部會鎖定該行
    Ok(())
}

// 階段二：Commit
pub async fn commit_db(&self, tx_id: Uuid) -> Result<(), String> {
    let commit_sql = format!("COMMIT PREPARED '{}'", tx_id);
    sqlx::query(&commit_sql).execute(&self.db_pool).await.map_err(|e| e.to_string())?;
    Ok(())
}

```

---

## 2PC 實踐的三大金律

在 Rust 中開發這種系統時，請記住以下原則：

1. **日誌先行 (WAL)：** 在回覆協調者 `Ready` 之前，務必先將決定寫入硬碟（例如 `sled`）。
2. **超時必備：** 無論是應用層的 `active_txs` 還是 DB 層的 `PREPARE`，一定要有清理機制，避免殭屍交易（Orphaned Transactions）耗盡資源。
3. **冪等補償：** 由於網路抖動，協調者可能會多次發送 `Commit`。你的 `commit_db` 函數應該處理「交易已提交」的錯誤，並視為成功。

這套機制雖然嚴密，但實作成本很高。**在你的場景中，是否真的需要這種「強一致性」？** 還是說，其實 **Saga 模式（最終一致性）** 已經足夠應付業務需求了？如果你有具體的業務場景（如電商、遊戲、或支付），我可以幫你分析哪種更合適。


