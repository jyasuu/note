I'll help you with PyTorch best practices covering all major features. Let me create a comprehensive guide that covers the essential practices for building robust, efficient PyTorch applications.This comprehensive guide covers PyTorch best practices across all major areas. Here are the key highlights:

**Core Areas Covered:**
- **Project Structure**: Proper organization and environment management
- **Data Handling**: Efficient datasets, data loaders, and preprocessing
- **Model Architecture**: Clean model design with proper initialization
- **Training**: Advanced techniques like mixed precision, gradient accumulation, and EMA
- **Optimization**: TorchScript, quantization, and distributed training
- **Debugging**: Profiling tools and anomaly detection
- **Testing**: Unit tests and validation strategies
- **Deployment**: Production-ready model serving

**Key Best Practices:**
1. Use `DataLoader` with proper `num_workers` and `pin_memory=True` for GPU training
2. Implement mixed precision training with `autocast()` and `GradScaler`
3. Apply gradient clipping and use `zero_grad(set_to_none=True)` for efficiency
4. Save complete checkpoints including optimizer and scheduler states
5. Use proper weight initialization and batch normalization
6. Implement comprehensive error handling and input validation
7. Profile your code to identify bottlenecks
8. Test models thoroughly with unit tests

Would you like me to expand on any specific area or provide more detailed examples for particular use cases like computer vision, NLP, or specific model architectures?

# PyTorch Best Practices - Comprehensive Guide

## 1. Project Structure & Environment Setup

### Recommended Project Structure
```
project/
├── src/
│   ├── models/
│   ├── data/
│   ├── utils/
│   └── training/
├── configs/
├── experiments/
├── tests/
├── requirements.txt
└── setup.py
```

### Environment Management
```python
# Use virtual environments
python -m venv pytorch_env
source pytorch_env/bin/activate  # Linux/Mac
# pytorch_env\Scripts\activate  # Windows

# Pin dependencies in requirements.txt
torch==2.1.0
torchvision==0.16.0
numpy==1.24.3
```

## 2. Data Handling Best Practices

### Dataset Implementation
```python
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

class CustomDataset(Dataset):
    def __init__(self, data_path, transform=None, cache_data=False):
        self.data_path = data_path
        self.transform = transform
        self.cache_data = cache_data
        self.data_cache = {} if cache_data else None
        self._load_metadata()
    
    def __len__(self):
        return len(self.metadata)
    
    def __getitem__(self, idx):
        if self.cache_data and idx in self.data_cache:
            return self.data_cache[idx]
        
        # Load data efficiently
        data = self._load_item(idx)
        
        if self.transform:
            data = self.transform(data)
        
        if self.cache_data:
            self.data_cache[idx] = data
        
        return data

# DataLoader best practices
def create_dataloader(dataset, batch_size, num_workers=4, pin_memory=True):
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,  # Faster GPU transfer
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=2 if num_workers > 0 else 2
    )
```

### Data Preprocessing & Augmentation
```python
from torchvision import transforms

# Efficient transforms pipeline
train_transforms = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Use transforms.Compose for better performance
# Avoid Python loops in transforms when possible
```

## 3. Model Architecture Best Practices

### Model Definition
```python
import torch.nn as nn
import torch.nn.init as init

class BestPracticeModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, dropout=0.1):
        super().__init__()
        
        # Use meaningful variable names
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(inplace=True),  # inplace=True saves memory
            nn.Dropout(dropout)
        )
        
        self.classifier = nn.Linear(hidden_size, num_classes)
        
        # Initialize weights properly
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
    
    def forward(self, x):
        # Add input validation in development
        assert x.dim() == 2, f"Expected 2D input, got {x.dim()}D"
        
        features = self.feature_extractor(x)
        output = self.classifier(features)
        return output

# Use nn.ModuleList for dynamic layers
class DynamicModel(nn.Module):
    def __init__(self, layer_sizes):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(layer_sizes[i], layer_sizes[i+1]) 
            for i in range(len(layer_sizes)-1)
        ])
```

### Memory Efficient Models
```python
# Use gradient checkpointing for large models
from torch.utils.checkpoint import checkpoint

class MemoryEfficientModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([...])
        
    def forward(self, x):
        # Use checkpointing for memory-intensive blocks
        for layer in self.layers:
            if self.training:
                x = checkpoint(layer, x)
            else:
                x = layer(x)
        return x
```

## 4. Training Loop Best Practices

### Comprehensive Training Setup
```python
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
from torch.cuda.amp import GradScaler, autocast
import logging

class Trainer:
    def __init__(self, model, train_loader, val_loader, device):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # Optimizer with weight decay
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=1e-3, 
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )
        
        # Learning rate scheduler
        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=100)
        
        # Mixed precision training
        self.scaler = GradScaler()
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # Metrics tracking
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device, non_blocking=True), \
                          target.to(self.device, non_blocking=True)
            
            # Zero gradients
            self.optimizer.zero_grad(set_to_none=True)  # More efficient
            
            # Mixed precision forward pass
            with autocast():
                output = self.model(data)
                loss = self.criterion(output, target)
            
            # Backward pass with gradient scaling
            self.scaler.scale(loss).backward()
            
            # Gradient clipping
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # Optimizer step
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            total_loss += loss.item()
            num_batches += 1
            
            # Log progress
            if batch_idx % 100 == 0:
                logging.info(f'Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        return total_loss / num_batches
    
    @torch.no_grad()
    def validate(self):
        self.model.eval()
        total_loss = 0
        correct = 0
        
        for data, target in self.val_loader:
            data, target = data.to(self.device, non_blocking=True), \
                          target.to(self.device, non_blocking=True)
            
            with autocast():
                output = self.model(data)
                loss = self.criterion(output, target)
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
        
        avg_loss = total_loss / len(self.val_loader)
        accuracy = 100. * correct / len(self.val_loader.dataset)
        
        return avg_loss, accuracy
```

### Advanced Training Techniques
```python
# Exponential Moving Average (EMA)
class EMA:
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]

# Gradient accumulation for large batch sizes
def train_with_grad_accumulation(model, dataloader, optimizer, accumulation_steps=4):
    model.train()
    optimizer.zero_grad()
    
    for i, (data, target) in enumerate(dataloader):
        output = model(data)
        loss = criterion(output, target) / accumulation_steps
        loss.backward()
        
        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
```

## 5. Model Optimization & Performance

### TorchScript & JIT Compilation
```python
# Model optimization with TorchScript
@torch.jit.script
def optimized_function(x: torch.Tensor) -> torch.Tensor:
    return torch.relu(x) * 0.5

# Trace model for deployment
model.eval()
example_input = torch.randn(1, 3, 224, 224)
traced_model = torch.jit.trace(model, example_input)
traced_model.save("model_traced.pt")

# Script model (more flexible)
scripted_model = torch.jit.script(model)
scripted_model.save("model_scripted.pt")
```

### Quantization
```python
import torch.quantization as quant

# Post-training quantization
def quantize_model(model, dataloader):
    # Prepare model
    model.eval()
    model_fused = torch.quantization.fuse_modules(model, [['conv', 'bn', 'relu']])
    
    # Configure quantization
    model_fused.qconfig = torch.quantization.get_default_qconfig('fbgemm')
    torch.quantization.prepare(model_fused, inplace=True)
    
    # Calibrate
    for data, _ in dataloader:
        model_fused(data)
    
    # Convert to quantized model
    model_quantized = torch.quantization.convert(model_fused, inplace=False)
    return model_quantized
```

### Distributed Training
```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_distributed(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def train_distributed():
    # Wrap model with DDP
    model = DDP(model, device_ids=[rank])
    
    # Use DistributedSampler
    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    train_loader = DataLoader(train_dataset, sampler=train_sampler)
    
    # Training loop remains mostly the same
    for epoch in range(num_epochs):
        train_sampler.set_epoch(epoch)  # Important for shuffling
        train_epoch()
```

## 6. Model Saving & Loading

### Checkpoint Management
```python
def save_checkpoint(model, optimizer, scheduler, epoch, loss, path):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'loss': loss,
        'pytorch_version': torch.__version__
    }
    torch.save(checkpoint, path)

def load_checkpoint(model, optimizer, scheduler, path):
    checkpoint = torch.load(path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    return checkpoint['epoch'], checkpoint['loss']

# Save only the model for inference
torch.save(model.state_dict(), 'model_weights.pth')
```

## 7. Debugging & Profiling

### Debugging Tools
```python
# Enable anomaly detection
torch.autograd.set_detect_anomaly(True)

# Check for NaN/Inf values
def check_gradients(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            if torch.isnan(param.grad).any():
                print(f"NaN gradient in {name}")
            if torch.isinf(param.grad).any():
                print(f"Inf gradient in {name}")

# Memory profiling
def profile_memory():
    print(f"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
    print(f"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
```

### Performance Profiling
```python
from torch.profiler import profile, record_function, ProfilerActivity

def profile_training():
    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], 
                record_shapes=True) as prof:
        with record_function("model_inference"):
            output = model(input_tensor)
        
        with record_function("loss_computation"):
            loss = criterion(output, target)
        
        with record_function("backward_pass"):
            loss.backward()
    
    print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

## 8. Testing & Validation

### Unit Testing
```python
import unittest

class TestModel(unittest.TestCase):
    def setUp(self):
        self.model = BestPracticeModel(10, 5, 2)
        self.input_tensor = torch.randn(4, 10)
    
    def test_forward_pass(self):
        output = self.model(self.input_tensor)
        self.assertEqual(output.shape, (4, 2))
    
    def test_gradient_flow(self):
        output = self.model(self.input_tensor)
        loss = output.sum()
        loss.backward()
        
        for param in self.model.parameters():
            self.assertIsNotNone(param.grad)
    
    def test_model_modes(self):
        self.model.eval()
        with torch.no_grad():
            eval_output = self.model(self.input_tensor)
        
        self.model.train()
        train_output = self.model(self.input_tensor)
        
        # Outputs might differ due to dropout, etc.
        self.assertEqual(eval_output.shape, train_output.shape)

if __name__ == '__main__':
    unittest.main()
```

## 9. Configuration Management

### Using Configuration Files
```python
import yaml
from dataclasses import dataclass

@dataclass
class TrainingConfig:
    batch_size: int = 32
    learning_rate: float = 1e-3
    num_epochs: int = 100
    weight_decay: float = 1e-4
    
    @classmethod
    def from_yaml(cls, path):
        with open(path, 'r') as f:
            config = yaml.safe_load(f)
        return cls(**config)

# config.yaml
"""
batch_size: 64
learning_rate: 0.001
num_epochs: 50
weight_decay: 0.0001
"""
```

## 10. Production Deployment

### Model Serving
```python
# Simple inference wrapper
class ModelPredictor:
    def __init__(self, model_path, device='cpu'):
        self.device = device
        self.model = self.load_model(model_path)
        self.model.eval()
    
    def load_model(self, path):
        model = BestPracticeModel()  # Initialize with correct args
        model.load_state_dict(torch.load(path, map_location=self.device))
        return model.to(self.device)
    
    @torch.no_grad()
    def predict(self, input_data):
        if isinstance(input_data, np.ndarray):
            input_tensor = torch.from_numpy(input_data).to(self.device)
        else:
            input_tensor = input_data.to(self.device)
        
        output = self.model(input_tensor)
        return output.cpu().numpy()
```

## Key Takeaways

1. **Structure**: Organize code into logical modules and use proper project structure
2. **Efficiency**: Use mixed precision, gradient accumulation, and efficient data loading
3. **Reproducibility**: Set seeds, save configurations, and version control everything
4. **Monitoring**: Track metrics, profile performance, and validate thoroughly
5. **Optimization**: Apply quantization, JIT compilation, and distributed training when needed
6. **Robustness**: Handle edge cases, validate inputs, and test comprehensively
7. **Maintainability**: Write clean, documented code with proper error handling




# PyTorch Best Practices Guide

This guide covers PyTorch best practices with practical examples to help you write efficient, maintainable code.

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, TensorDataset
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
```

## 1. Device Configuration

```python
# Set device agnostic code
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# For multiple GPUs
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs")
```

## 2. Tensor Operations Best Practices

```python
# Use torch functions instead of numpy for GPU acceleration
def tensor_operations():
    # Create tensors directly on target device
    x = torch.tensor([1, 2, 3], device=device, dtype=torch.float32)
    
    # Use in-place operations sparingly (only when necessary)
    x.add_(1)  # In-place
    x = x + 1  # Prefer this (creates new tensor)
    
    # Use torch.where for conditional operations
    y = torch.where(x > 2, x, torch.zeros_like(x))
    
    # Avoid CPU-GPU transfers
    x_cpu = x.cpu()  # Only when necessary
    
    return y

# Use vectorized operations instead of loops
def vectorized_operations():
    # Bad: Looping through elements
    x = torch.randn(1000, 1000)
    # Good: Vectorized operations
    result = x * 2 + 1
    return result
```

## 3. Dataset and DataLoader Best Practices

```python
# Custom Dataset implementation
class CustomDataset(Dataset):
    def __init__(self, data, targets, transform=None):
        self.data = data
        self.targets = targets
        self.transform = transform
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        label = self.targets[idx]
        
        if self.transform:
            sample = self.transform(sample)
            
        return sample, label

# Data loading best practices
def setup_data_loaders(batch_size=32):
    # Example data
    X = torch.randn(1000, 1, 28, 28)  # 1000 samples, 1 channel, 28x28
    y = torch.randint(0, 10, (1000,))
    
    dataset = TensorDataset(X, y)
    
    # Use appropriate number of workers
    num_workers = min(4, os.cpu_count() - 1)  # Leave one CPU core free
    
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True  # Faster data transfer to GPU
    )
    
    return loader
```

## 4. Model Definition Best Practices

```python
# Define model with proper initialization
class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.5)
        
        # Initialize weights properly
        self._initialize_weights()
        
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

# Use nn.Sequential when appropriate
def create_cnn_model():
    return nn.Sequential(
        nn.Conv2d(1, 32, 3, padding=1),
        nn.BatchNorm2d(32),
        nn.ReLU(inplace=True),
        nn.MaxPool2d(2),
        nn.Conv2d(32, 64, 3, padding=1),
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.MaxPool2d(2),
        nn.Flatten(),
        nn.Linear(64 * 7 * 7, 128),
        nn.ReLU(inplace=True),
        nn.Dropout(0.5),
        nn.Linear(128, 10)
    )
```

## 5. Training Loop Best Practices

```python
def train_model(model, train_loader, val_loader, epochs=10):
    model = model.to(device)
    
    # Use appropriate optimizer and scheduler
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.1, patience=2
    )
    criterion = nn.CrossEntropyLoss()
    
    # Set up TensorBoard logging
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    writer = SummaryWriter(f'runs/model_{timestamp}')
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad(set_to_none=True)  # More efficient memory usage
            
            output = model(data)
            loss = criterion(output, target)
            
            loss.backward()
            
            # Gradient clipping to prevent exploding gradients
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            
            # Log batch metrics
            if batch_idx % 100 == 0:
                writer.add_scalar('training_loss_batch', loss.item(), 
                                 epoch * len(train_loader) + batch_idx)
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                val_loss += loss.item()
                
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        # Calculate metrics
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        # Update learning rate
        scheduler.step(avg_val_loss)
        
        # Log epoch metrics
        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)
        writer.add_scalar('train_loss_epoch', avg_train_loss, epoch)
        writer.add_scalar('val_loss_epoch', avg_val_loss, epoch)
        writer.add_scalar('accuracy', accuracy, epoch)
        
        # Save best model
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_val_loss,
            }, 'best_model.pth')
        
        print(f'Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f}, '
              f'Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%')
    
    writer.close()
    return model
```

## 6. Mixed Precision Training

```python
def train_with_mixed_precision(model, train_loader, epochs=10):
    from torch.cuda.amp import autocast, GradScaler
    
    scaler = GradScaler()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    model = model.to(device)
    
    for epoch in range(epochs):
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            
            # Mixed precision forward pass
            with autocast():
                output = model(data)
                loss = criterion(output, target)
            
            # Backward pass with scaling
            scaler.scale(loss).backward()
            
            # Unscale gradients and clip
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            # Update weights
            scaler.step(optimizer)
            scaler.update()
```

## 7. Distributed Training

```python
def setup_distributed_training():
    # Initialize distributed backend
    torch.distributed.init_process_group(backend='nccl')
    
    local_rank = torch.distributed.get_rank()
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)
    
    # Create model and move to device
    model = NeuralNet(784, 512, 10).to(device)
    
    # Wrap model for distributed training
    model = nn.parallel.DistributedDataParallel(
        model,
        device_ids=[local_rank],
        output_device=local_rank
    )
    
    return model, device
```

## 8. Model Saving and Loading

```python
def save_checkpoint(model, optimizer, epoch, loss, path='checkpoint.pth'):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, path)

def load_checkpoint(model, optimizer, path='checkpoint.pth'):
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    
    return model, optimizer, epoch, loss

# For inference
def load_model_for_inference(model_class, path, **model_args):
    model = model_class(**model_args)
    model.load_state_dict(torch.load(path, map_location=device))
    model.to(device)
    model.eval()  # Set to evaluation mode
    return model
```

## 9. Inference Best Practices

```python
def inference(model, data_loader):
    model.eval()  # Always set to evaluation mode
    predictions = []
    
    with torch.no_grad():  # Disable gradient computation
        for data in data_loader:
            data = data.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            predictions.append(pred.cpu())
    
    return torch.cat(predictions, dim=0)

# Use torch.inference_mode for even better performance in PyTorch 1.9+
def inference_optimized(model, data_loader):
    model.eval()
    predictions = []
    
    with torch.inference_mode():
        for data in data_loader:
            data = data.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            predictions.append(pred.cpu())
    
    return torch.cat(predictions, dim=0)
```

## 10. Debugging and Profiling

```python
def debug_model():
    model = NeuralNet(784, 512, 10)
    
    # Add hooks for debugging
    def forward_hook(module, input, output):
        print(f"{module.__class__.__name__} output shape: {output.shape}")
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            module.register_forward_hook(forward_hook)
    
    # Use torch.autograd.gradcheck for gradient verification
    input = torch.randn(10, 784, requires_grad=True)
    test = torch.autograd.gradcheck(lambda x: model(x).sum(), input)
    print(f"Gradient check passed: {test}")
    
    # Profile model
    with torch.profiler.profile(
        activities=[torch.profiler.ProfilerActivity.CPU,
                   torch.profiler.ProfilerActivity.CUDA],
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
    ) as prof:
        for _ in range(10):
            x = torch.randn(32, 784)
            model(x)
            prof.step()
```

## 11. Complete Example

```python
def main():
    # Setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # Create sample data
    X_train = torch.randn(1000, 784)
    y_train = torch.randint(0, 10, (1000,))
    X_val = torch.randn(200, 784)
    y_val = torch.randint(0, 10, (200,))
    
    # Create data loaders
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # Create model
    model = NeuralNet(784, 512, 10)
    
    # Train model
    trained_model = train_model(model, train_loader, val_loader, epochs=5)
    
    # Save model
    torch.save(trained_model.state_dict(), "trained_model.pth")
    
    # Load model for inference
    loaded_model = NeuralNet(784, 512, 10)
    loaded_model.load_state_dict(torch.load("trained_model.pth", map_location=device))
    loaded_model.to(device)
    loaded_model.eval()
    
    # Test inference
    test_data = torch.randn(10, 784).to(device)
    with torch.no_grad():
        output = loaded_model(test_data)
        predictions = output.argmax(dim=1)
        print(f"Predictions: {predictions.cpu().numpy()}")

if __name__ == "__main__":
    main()
```

## Key Best Practices Summary:

1. **Device Management**: Write device-agnostic code
2. **Tensor Operations**: Use vectorized operations and avoid unnecessary CPU-GPU transfers
3. **Data Loading**: Use DataLoader with appropriate num_workers and pin_memory
4. **Model Architecture**: Use proper weight initialization and normalization
5. **Training**: Implement validation, checkpointing, and learning rate scheduling
6. **Mixed Precision**: Use for faster training on compatible hardware
7. **Distributed Training**: Scale training across multiple GPUs/nodes
8. **Saving/Loading**: Save both model and optimizer state for resuming training
9. **Inference**: Use `torch.no_grad()` or `torch.inference_mode()`
10. **Debugging**: Use hooks and profiler to identify bottlenecks

This covers the essential PyTorch best practices. Remember to always profile your code to identify actual bottlenecks rather than guessing!


