Yes, I'm familiar with common Kubernetes anti-patterns and mistakes. Here are some of the most frequent ones:

**Resource requests and limits**: Not setting resource requests and limits, or setting them incorrectly, is extremely common. Without requests, the scheduler can't make informed placement decisions. Without limits, a single pod can consume all node resources and starve others. People often set them too high or forget them entirely.

**Using latest tag**: Deploying with `image: myapp:latest` makes it impossible to track which version is running and causes unpredictable behavior during rollouts. Always use specific version tags.

**No health checks**: Missing liveness and readiness probes means Kubernetes doesn't know when a pod is actually ready to serve traffic or when it's in a broken state, so it keeps routing traffic to unhealthy instances.

**Monolithic containers**: Putting too much into a single container defeats Kubernetes' ability to scale and manage components independently. One slow process can bring down the whole pod.

**Ignoring security contexts**: Not setting security policies, running containers as root, or not restricting capabilities creates unnecessary security risks.

**Poor namespace usage**: Treating namespaces as just organizational tools rather than security and resource boundaries. They're useful for multi-tenancy and preventing accidental cross-team changes.

**No persistent volume strategy**: Either losing state that should persist, or trying to persist state in containers that shouldn't have it. Misunderstanding when to use volumes, StatefulSets, or databases.

**Tight coupling to nodes**: Using nodeSelector or nodeAffinity too liberally reduces portability and makes clusters less flexible. Reserved capacity on specific nodes is often unnecessary.

**Ignoring resource quotas**: Without quotas on namespaces, a runaway deployment can consume cluster resources and break other workloads.

**ConfigMaps and Secrets misuse**: Storing sensitive data in ConfigMaps, using them for large files, or not using Secrets properly for credentials and keys.

**No network policies**: Allowing all traffic between pods by default creates unnecessary blast radius during compromises.

Are there specific areas you'd like to dive deeper into?

Great question. Let me walk through some realistic scenarios where this goes wrong:

**Scenario 1: Losing data on pod restart**

Someone deploys a database or stateful application without persistent volumes. The application writes data to a local directory like `/var/lib/mysql` or `/data`. This works fine until the pod crashes, gets evicted, or the node fails. When Kubernetes reschedules the pod on another node, all that data is gone. The new pod starts fresh with an empty directory.

```yaml
# WRONG - data is lost on pod restart
apiVersion: v1
kind: Pod
metadata:
  name: mysql
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    # No volume specified - data lives only in the container
```

**Scenario 2: Misunderstanding emptyDir**

Developers sometimes use `emptyDir` volumes thinking they provide persistence. They don't—`emptyDir` is created when the pod is scheduled and deleted when the pod terminates. It's only useful for temporary cache data or shared storage between containers in the same pod, not for important data.

```yaml
# WRONG - emptyDir is temporary!
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    emptyDir: {}  # Gets deleted when pod dies
```

**Scenario 3: Using Deployment for stateful apps**

Someone uses a Deployment to run a database or application that needs persistent state. Deployments can spin up multiple replicas on different nodes, and pods can be recreated anywhere. If you have 3 replicas of a database each writing to different volumes, you end up with three separate databases that are out of sync—that's not what you want.

```yaml
# WRONG - Deployment with stateful app
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
spec:
  replicas: 3  # Multiple replicas of a DB is wrong!
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: postgres-pvc
```

The right approach here is a **StatefulSet**, which ensures each replica has its own stable identity and persistent volume.

**Scenario 4: Correct approach with StatefulSet**

```yaml
# RIGHT - StatefulSet for stateful applications
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 1  # Single instance for a database
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
```

StatefulSets create a PVC for each replica (`postgres-0`, `postgres-1`, etc.), ensuring stable identities and persistent state tied to specific pods.

**Scenario 5: Using wrong storage for use case**

Sometimes people pair persistent volumes with the wrong application pattern. For example, using a single shared PVC across multiple pod replicas of a stateless application is wasteful. Stateless apps like web servers don't need persistence at all. But a logging system or cache that needs to survive pod restarts does.

**The key distinctions:**

- **Stateless apps** (web servers, API services): No persistent volume needed. Use Deployment.
- **Stateful apps** (databases, message queues): Need persistent storage. Use StatefulSet with PVCs.
- **Temporary storage**: Use emptyDir only for cache or inter-container communication.
- **Shared data**: Use appropriate storage classes and access modes (ReadWriteOnce vs ReadWriteMany).

Does this clarify the pattern better?
