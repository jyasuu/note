To check available resources for scheduling in Kubernetes, you generally want to inspect:

1. **Node allocatable resources** (CPU, memory)
2. **Current pod usage** on each node
3. **Remaining (available) capacity** = allocatable - usage

Here are several approaches depending on the detail and tooling you want:

---

## üîç Quick CLI Check

### 1. **Check Node Allocatable Resources**

```bash
kubectl describe nodes | grep -A5 "Allocatable"
```

### 2. **Check Node Resource Usage**

You can use:

```bash
kubectl top nodes
```

You‚Äôll get output like:

```bash
NAME         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
node-1       450m         22%    1024Mi          60%
```

### 3. **Check Pod Requests**

```bash
kubectl describe node <node-name>
```

Look under `Allocated resources:` for:

```
Resource           Requests   Limits
--------           --------   ------
cpu                800m       1000m
memory             1Gi        2Gi
```

---

## üß∞ Advanced: Get Free Resource via Script

You can run this shell command to estimate free allocatable CPU/Memory per node:

```bash
kubectl get nodes -o json | jq '.items[] | {
  name: .metadata.name,
  allocatable: .status.allocatable,
  capacity: .status.capacity
}'
```

Or for a more usable summary:

```bash
kubectl get nodes -o custom-columns="NAME:.metadata.name,CPU:.status.allocatable.cpu,MEM:.status.allocatable.memory"
```

---

## üß† Scheduling Decision Insights

Kubernetes schedules based on:

* Node taints/tolerations
* Resource requests (not limits)
* Affinity rules
* PodDisruptionBudgets
* NodeSelectors / NodeAffinity

If you're debugging why a pod is **not scheduling**, try:

```bash
kubectl describe pod <pod-name>
```

Look under `Events:` for messages like:

```
0/3 nodes are available: 1 Insufficient memory, 2 MatchNodeSelector.
```

---

## üì¶ Use Metrics Server or Kube-State-Metrics

If you have `metrics-server` or `kube-state-metrics` installed, tools like `kubectl top` or Prometheus/Grafana can give you better real-time scheduling availability views.

---

## ‚úÖ Summary

| Command                        | Purpose                           |
| ------------------------------ | --------------------------------- |
| `kubectl top nodes`            | See current usage                 |
| `kubectl describe node <node>` | See detailed resource allocations |
| `kubectl get nodes -o wide`    | Quick overview of nodes           |
| `kubectl describe pod <pod>`   | See scheduling events/issues      |

---

If you tell me what you're trying to schedule (CPU/Memory/Taints/Zone requirements), I can help simulate a schedule decision or build a filter for available nodes.




Yes ‚Äî setting **`resources.requests`** and **`resources.limits`** in Kubernetes is critical for performance, reliability, and cluster efficiency. Here's a **professional guide** tailored to your experience level.

---

## üß† Core Concepts

* **`requests`** = Minimum guaranteed resources. Used by the scheduler.
* **`limits`** = Maximum resource a container can use. Enforced at runtime.

If a container exceeds its **memory limit**, it's **OOMKilled**.
If it exceeds **CPU limit**, it's **throttled** (not killed).

---

## üéØ Best Practices

### 1. **Set Both `requests` and `limits`**

```yaml
resources:
  requests:
    cpu: "250m"
    memory: "512Mi"
  limits:
    cpu: "500m"
    memory: "1Gi"
```

* Keep `requests` == `limits` if you need consistent performance (e.g., latency-sensitive workloads).
* Allow headroom (requests < limits) for batch or bursty workloads.

---

### 2. **Profile Your Workload**

Use tools like:

* `kubectl top pod`
* Prometheus + Grafana dashboards
* Historical usage metrics

**Example:** After observing that your pod usually uses 300m CPU and 700Mi memory:

```yaml
requests:
  cpu: "300m"
  memory: "700Mi"
limits:
  cpu: "500m"
  memory: "1Gi"
```

---

### 3. **Avoid Over-Provisioning**

* Overestimating `requests` blocks others from scheduling.
* Underestimating `limits` can kill or throttle your workload.

üëé Bad (over-requesting):

```yaml
requests:
  cpu: "2"
  memory: "4Gi"
```

üëç Better (based on real usage):

```yaml
requests:
  cpu: "500m"
  memory: "1Gi"
```

---

### 4. **Use Vertical/Horizontal Autoscaling (VPA/HPA)**

* **HPA**: Adjusts replicas based on CPU/memory/metrics.
* **VPA**: Recommends/adjusts requests/limits per pod.

> ‚ö†Ô∏è HPA and VPA can't manage the same resource at the same time.

---

### 5. **Set Default Requests/Limits via LimitRange**

You can define defaults at the namespace level to avoid unbounded pods:

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: default-resource-limits
spec:
  limits:
  - default:
      cpu: 1
      memory: 1Gi
    defaultRequest:
      cpu: 500m
      memory: 512Mi
    type: Container
```

---

## üß™ Rule of Thumb

| Type of App         | CPU Request | Memory Request | Limit Strategy                  |
| ------------------- | ----------- | -------------- | ------------------------------- |
| Web App (stateless) | 200‚Äì500m    | 256Mi‚Äì1Gi      | Set limits for safety           |
| Java App (stateful) | 500m‚Äì2      | 1Gi‚Äì4Gi        | Limits ‚â• JVM max heap           |
| Batch Jobs          | Low (100m)  | Depends        | Set limits > requests           |
| DB/Cache (stateful) | High        | High           | No CPU limit (avoid throttling) |

---

## üõ°Ô∏è Pro Tip for Java Apps

Set:

* `memory.requests` = X
* `memory.limits` = X
* JVM heap size = X \* 0.75 (approx)

This avoids being OOMKilled due to container limits.

---

If you give me an example workload (e.g., Go API server, Java batch job), I can suggest more specific values.
