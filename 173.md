Absolutely—here’s a practical, battle‑tested guide to using PostgreSQL **temporary tables** effectively, with patterns, pitfalls, and snippets you can drop into your workflow.

***

## What a temp table is (quick refresher)

*   **Session‑local**: `TEMP`/`TEMPORARY` tables live only for your session (or transaction if you use `ON COMMIT DROP`).
*   **Schema**: They’re created under the session’s `pg_temp` schema (not visible to other sessions).
*   **Performance**: Great for complex ETL steps, de-duplication, breaking apart big joins, or caching intermediate results.
*   **Isolation**: Safe in concurrent environments—each session gets its own copy.

***

## Core best practices

### 1) Prefer `CREATE TEMP TABLE … AS` (CTAS) for speed

```sql
-- Fast load from a SELECT
CREATE TEMP TABLE tmp_orders AS
SELECT o.*
FROM orders o
WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days';

-- If you need constraints, add them after CTAS:
ALTER TABLE tmp_orders ADD PRIMARY KEY (order_id);
CREATE INDEX ON tmp_orders (customer_id);
ANALYZE tmp_orders;
```

*   CTAS avoids row‑by‑row INSERT overhead.
*   Add indexes **after** loading to reduce write amplification.

### 2) Scope explicitly with `ON COMMIT`

```sql
-- Drops at transaction end—great for functions or transactional ETL
CREATE TEMP TABLE tmp_work ON COMMIT DROP AS
SELECT ...;

-- Preserve across transactions in session
CREATE TEMP TABLE tmp_cache ON COMMIT PRESERVE ROWS AS
SELECT ...;
```

*   Use `ON COMMIT DROP` when you want guaranteed cleanup at the end of a transaction.
*   Use `ON COMMIT PRESERVE ROWS` for multi‑step interactive sessions.

### 3) Name clearly and avoid collisions

*   Use a prefix like `tmp_`, `stg_`, or `work_`.
*   If creating many in the same session, add logical suffixes: `tmp_users_stage1`, `tmp_users_dedup`.

### 4) Index only what you need

*   Create indexes on columns used in **joins, WHERE, GROUP BY, DISTINCT** of *downstream* steps.
*   Example:

```sql
CREATE INDEX tmp_orders_cust_idx ON tmp_orders (customer_id);
CREATE INDEX tmp_orders_date_idx ON tmp_orders (order_date);
ANALYZE tmp_orders;  -- update stats to guide planner
```

*   Don’t over‑index: each index adds write cost and memory while building.

### 5) Analyze after bulk load

*   `ANALYZE` temp tables to give the planner fresh stats (especially after CTAS).
*   If the table is tiny (< few thousand rows), the planner often does fine without—but it’s cheap to run.

### 6) Use `UNLOGGED` tables when you need persistence in session but faster writes

*   `UNLOGGED` tables are **not temporary**, but they skip WAL (faster writes, lost on crash).

```sql
CREATE UNLOGGED TABLE work_buffer AS
SELECT ...;
```

*   Good for staging in long ETL jobs when you need to reuse across transactions within the same DB but don’t need crash safety.
*   Not visible only to your session—be mindful of concurrency.

### 7) Prefer CTEs or subqueries for simple one‑off steps

If your “temp” is just a single intermediate result used once:

```sql
WITH recent_orders AS (
  SELECT * FROM orders WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'
)
SELECT ...
FROM recent_orders r
JOIN customers c ON c.id = r.customer_id;
```

*   CTEs keep things inline; temp tables shine when **materializing** heavy results to reuse in multiple queries or to add **indexes**.

### 8) Clean up explicitly in scripts

Even though temp tables drop on session end, be explicit in long‑running tools:

```sql
DROP TABLE IF EXISTS tmp_orders;
```

*   Helps avoid stale names if your script reconnects or runs in pooled sessions.

### 9) Avoid temp tables inside long transactions unless intended

*   Long transactions hold locks and affect vacuum. If you need many steps, consider breaking into smaller transactions and using `ON COMMIT PRESERVE ROWS` or `UNLOGGED` staging.

### 10) Watch memory & temp file usage

*   Big sorts/joins on temp tables can spill to disk. Keep an eye on:
    *   `work_mem` (per sort/hash) – tune cautiously for batch jobs.
    *   `temp_file_limit` – prevent runaway disk usage.
*   For hashing/aggregation heavy workloads, increasing `work_mem` for the session can help:

```sql
SET LOCAL work_mem = '256MB';  -- inside a transaction or session for the heavy step
```

***

## Common patterns

### Pattern A: Staging + indexing + downstream joins

```sql
BEGIN;

-- Step 1: Stage recent order lines
CREATE TEMP TABLE tmp_recent_lines AS
SELECT ol.*
FROM order_lines ol
JOIN orders o ON o.id = ol.order_id
WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days';

-- Step 2: Index on join keys and filters
CREATE INDEX ON tmp_recent_lines (product_id);
CREATE INDEX ON tmp_recent_lines (order_id);
ANALYZE tmp_recent_lines;

-- Step 3: Use staged data in a complex aggregation
SELECT p.category_id, SUM(ol.quantity) AS qty_30d
FROM tmp_recent_lines ol
JOIN products p ON p.id = ol.product_id
GROUP BY p.category_id;

COMMIT;
```

### Pattern B: Deduplication pipeline

```sql
CREATE TEMP TABLE tmp_users_raw AS
SELECT *
FROM incoming_users;  -- e.g., a loaded staging source

-- Normalize and dedup
CREATE TEMP TABLE tmp_users_clean AS
SELECT DISTINCT ON (email_lower)
  id,
  LOWER(email) AS email_lower,
  TRIM(name) AS name,
  created_at
FROM tmp_users_raw
ORDER BY email_lower, created_at DESC;

CREATE INDEX ON tmp_users_clean (email_lower);
ANALYZE tmp_users_clean;

-- Upsert into target
INSERT INTO users AS u (email, name, created_at)
SELECT email_lower, name, created_at
FROM tmp_users_clean
ON CONFLICT (email) DO UPDATE
SET name = EXCLUDED.name,
    created_at = GREATEST(u.created_at, EXCLUDED.created_at);
```

### Pattern C: Breaking apart a heavy multi‑join

Materialize parts to control join order and reuse:

```sql
-- Pre-filter big fact table
CREATE TEMP TABLE tmp_fact AS
SELECT *
FROM fact_sales
WHERE sale_date BETWEEN DATE '2025-11-01' AND DATE '2025-11-30';

CREATE INDEX ON tmp_fact (customer_id);
CREATE INDEX ON tmp_fact (product_id);
ANALYZE tmp_fact;

-- Join to smaller dims
SELECT f.*, c.region, p.category
FROM tmp_fact f
JOIN dim_customers c ON c.id = f.customer_id
JOIN dim_products  p ON p.id = f.product_id;
```

***

## Transaction and concurrency tips

*   Temp tables are **private**: you can safely use the same name across sessions.
*   Locks: temp tables still acquire locks, but only visible in your session.
*   Functions: inside `plpgsql`, `ON COMMIT DROP` helps ensure cleanup.

```sql
DO $$
DECLARE
BEGIN
  CREATE TEMP TABLE tmp_calc ON COMMIT DROP AS
  SELECT ...
  FROM ...

  -- process
  UPDATE tmp_calc SET ...;

  -- use it
  INSERT INTO final_table (...)
  SELECT ...
  FROM tmp_calc;
END $$;
```

***

## When NOT to use temp tables

*   **Single-use, simple intermediate** → use a CTE/subquery.
*   **Need sharing across sessions** → use `UNLOGGED` or regular tables in a staging schema.
*   **High churn in pooled environments** → temp table creation can cause catalog bloat; consider fewer, reusable staging tables.

***

## Performance checklist (Do/Don’t)

**Do**

*   Use CTAS to load quickly.
*   Add only necessary indexes *after* load.
*   `ANALYZE` after bulk operations.
*   Tune `work_mem` locally for heavy joins/aggregations.
*   Use `ON COMMIT DROP` in transactional ETL/function blocks.
*   Consider `UNLOGGED` for fast, non‑WAL staging across steps.

**Don’t**

*   Over‑index temp tables.
*   Keep temp tables alive across long transactions without reason.
*   Forget cleanup in scripts using connection pools.
*   Assume temp tables are faster than CTEs for trivial cases.

***

## Troubleshooting & observability

*   See temp table definitions in your session:

```sql
SELECT relname, relkind
FROM pg_class
WHERE relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'pg_temp');
```

*   Check temp file usage per backend via logs:
    *   Enable `log_temp_files = 0` (or a threshold) to log spills.
*   Watch query plans:

```sql
EXPLAIN (ANALYZE, BUFFERS) SELECT ... FROM tmp_table ...;
```

***

## Advanced: parallel + temp tables

*   Parallel workers cannot write to your temp table, but they can read if created beforehand and plan allows.
*   For parallel‑friendly pipelines, materialize the large filter first, index it, then do read‑heavy downstream steps.

***

## Ready‑to‑use template

```sql
-- Session setup
BEGIN;
SET LOCAL work_mem = '256MB';

-- Stage
DROP TABLE IF EXISTS tmp_stage;
CREATE TEMP TABLE tmp_stage AS
SELECT ...
FROM source
WHERE ...;

-- Indexes for downstream operations
CREATE INDEX tmp_stage_key_idx ON tmp_stage (key_col);
CREATE INDEX tmp_stage_filter_idx ON tmp_stage (filter_col);
ANALYZE tmp_stage;

-- Processing step
CREATE TEMP TABLE tmp_proc AS
SELECT ...
FROM tmp_stage s
JOIN dim d ON d.id = s.key_col;

CREATE INDEX ON tmp_proc (join_key);
ANALYZE tmp_proc;

-- Final write
INSERT INTO target_table (...)
SELECT ...
FROM tmp_proc;

COMMIT;
```

***

## Quick guardrails for production ETL

*   Use a dedicated **staging schema** for `UNLOGGED`/regular materializations; use temp tables for per‑job private steps.
*   Wrap each logical phase in a transaction; keep temp tables `ON COMMIT DROP` where possible.
*   Log spills and measure impact; adjust `work_mem` only locally.
*   Maintain `pg_bouncer`/pooler awareness: temp tables are per session—avoid sharing steps across pooled transactions unless you pin sessions.

***

If you share a bit more about your use case (ETL vs. reporting vs. ad‑hoc analysis, approximate row counts, and Postgres version), I can tailor a concrete pattern and sizing (indexes, `work_mem`, and plan hints) for your scenario.




Great question! The **lifetime of a PostgreSQL temporary table** depends on how you create it and the session/transaction context:

***

### ✅ **Default Lifetime**

*   A `TEMP` or `TEMPORARY` table exists **only for the duration of the session** that created it.
*   When the session ends (connection closes), the temp table is automatically dropped.

***

### ✅ **Transaction Control with `ON COMMIT`**

You can control lifetime more precisely using `ON COMMIT`:

*   **`ON COMMIT DROP`**
    *   Drops the temp table **at the end of the current transaction**.
    *   Useful for short-lived intermediate steps inside a transaction or stored procedure.
    ```sql
    CREATE TEMP TABLE tmp_data ON COMMIT DROP AS
    SELECT ...;
    ```

*   **`ON COMMIT DELETE ROWS`**
    *   Keeps the table structure but clears all rows after each commit.
    *   Rarely used, but handy for iterative inserts in the same session.
    ```sql
    CREATE TEMP TABLE tmp_buffer ON COMMIT DELETE ROWS (...);
    ```

*   **`ON COMMIT PRESERVE ROWS`** (default)
    *   Keeps the table and its data across commits until the session ends.

***

### ✅ **Key Points**

*   Temp tables are **session-scoped**, not visible to other sessions.
*   They **do not survive a reconnect** (e.g., in connection pools like PgBouncer, this matters).
*   If you need persistence beyond a session, use:
    *   **UNLOGGED tables** (faster, skip WAL, survive session end but not crash)
    *   **Regular tables in a staging schema** for multi-session workflows.

***

### ✅ **Best Practice for Lifetime**

*   For **ETL or stored procedures**: use `ON COMMIT DROP` to ensure cleanup.
*   For **interactive analysis**: default (`ON COMMIT PRESERVE ROWS`) is fine.
*   For **connection pools**: avoid temp tables unless you pin sessions, because pooled connections may reuse sessions unexpectedly.

***

Do you want me to **draw a quick lifecycle diagram** showing how temp tables behave across **session vs transaction vs commit**? Or should I give you a **code template for safe cleanup in pooled environments**?



Here’s a pragmatic, engineer‑friendly guide to **PostgreSQL `UNLOGGED` tables**—what they are, when (and how) to use them, and the gotchas you should plan for.

***

## What is an UNLOGGED table?

*   **No WAL writes:** Operations on `UNLOGGED` tables (and their indexes) do not go to the Write‑Ahead Log (WAL). This reduces I/O and can significantly speed up heavy insert/update workloads.
*   **Crash behavior:** After a **crash** (not a clean shutdown), PostgreSQL **truncates** `UNLOGGED` tables to empty on restart. Schema remains, data is wiped.
*   **Replication/backup implications:**
    *   **Physical/streaming replication:** Standby nodes won’t have the data reliably (no WAL), so contents diverge or appear empty after crash.
    *   **Logical replication/publication:** Changes typically are **not replicated** from `UNLOGGED` tables.
    *   **Base backups:** Schema is included, but data reliability is not guaranteed across crash recovery.
*   **Indexes:** Indexes on `UNLOGGED` tables are also **unlogged** automatically.
*   **Constraints/Referential integrity:** You can use constraints, but **do not** reference `UNLOGGED` tables from `LOGGED` tables (FKs between logged ↔ unlogged are disallowed). Keep integrity **within** the same persistence class.

***

## Good use cases

Use `UNLOGGED` only if the data is **ephemeral or reproducible**:

*   **Staging / scratch tables** for ETL/ELT pipelines.
*   **Caching layers** (e.g., computed results you can rebuild).
*   **Bulk load buffers** before merging into logged/replicated tables.
*   **High‑rate queue/metrics ingestion** where occasional loss is tolerable.
*   **Session state** that can be regenerated.

Avoid for anything **business‑critical**, audit trails, or regulatory data.

***

## Core best practices

### 1) Create explicitly as UNLOGGED

```sql
CREATE UNLOGGED TABLE staging_events (
  event_id   bigint GENERATED ALWAYS AS IDENTITY,
  source     text NOT NULL,
  payload    jsonb NOT NULL,
  received_at timestamptz NOT NULL DEFAULT now(),
  PRIMARY KEY (event_id)
);
```

### 2) Keep integrity boundaries simple

*   **Don’t mix persistence** across FKs: no FK from a logged table to an unlogged table (and vice versa).
*   If integrity is required, do it **inside** the unlogged dataset or via post‑processing when promoting to logged storage.

### 3) Crash tolerance plan

*   Assume **data may disappear after crash**.
*   Keep a **rebuild path** (e.g., re‑ingest from source, replay Kafka, reload files).
*   Consider **periodic promotion** to logged tables for checkpoints.

### 4) Replication/HA strategy

*   If you rely on streaming replication or hot standbys, **don’t depend** on the contents of unlogged tables on standbys.
*   For HA failover, ensure your system can **repopulate** them.

### 5) Bulk‑load optimization

Combine `UNLOGGED` with:

*   `COPY` instead of many `INSERT`s.
*   `synchronous_commit = off` for the session, if acceptable.
*   Defer index creation until after large loads, or `CREATE INDEX CONCURRENTLY` if you must keep the table online (note: concurrent builds still need careful planning).
*   Disable/avoid heavy constraints during load; validate after.

Example:

```sql
BEGIN;
SET LOCAL synchronous_commit = OFF;

COPY staging_events (source, payload, received_at)
FROM '/data/import/events.jsonl'
WITH (FORMAT text);

COMMIT;
```

### 6) Statistics & maintenance

*   Run `ANALYZE` after large loads to update planner stats:
    ```sql
    ANALYZE staging_events;
    ```
*   `VACUUM`/autovacuum still applies—plan bloat control like usual.
*   Monitor with `pg_class.relkind`, `pg_class.relpersistence` (`'u'` for unlogged).

### 7) Toggle persistence carefully

You can switch a table:

```sql
ALTER TABLE staging_events SET LOGGED;   -- rewrites table, exclusive lock
ALTER TABLE staging_events SET UNLOGGED; -- likewise
```

**Caveats:**

*   This is a **table rewrite**; expect I/O and blocking.
*   Ensure maintenance windows or use partition‑level toggling (see below).

### 8) Partitioning strategy

*   Parent and partitions can have **different** persistence, but keep it consistent unless you have a specific plan (e.g., recent partitions unlogged for speed; older ones logged).
*   Example:

```sql
CREATE TABLE events_p (
  event_id bigint GENERATED BY DEFAULT AS IDENTITY,
  event_ts timestamptz NOT NULL,
  payload jsonb NOT NULL
) PARTITION BY RANGE (event_ts);

CREATE UNLOGGED TABLE events_p_2025_12 PARTITION OF events_p
FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');

CREATE TABLE events_p_2025_11 PARTITION OF events_p
FOR VALUES FROM ('2025-11-01') TO ('2025-12-01'); -- logged
```

Use this to **accelerate hot data** while keeping historical data **durable/replicated**.

### 9) Backup/restore considerations

*   Don’t count on unlogged contents being present after crash or on replicas.
*   If you must preserve snapshots, periodically **promote & snapshot**:
    ```sql
    -- promote the current active unlogged partition:
    ALTER TABLE events_p_2025_12 SET LOGGED;
    -- take backup / create a materialized view / export snapshot
    ```

### 10) Access patterns & indexing

*   Index only what you need; avoid over‑indexing hot write paths.
*   If your workload is mostly **append‑only**, consider:
    *   Fillfactor tuning (e.g., `WITH (fillfactor=95)`).
    *   Avoid frequent updates; prefer upserts with minimal churn.
*   For large `COPY` loads, create indexes **after** the load.

***

## Operational checklist

*   [ ] Data is non‑critical and **recreatable** after crash.
*   [ ] **No cross‑persistence FKs** (logged ↔ unlogged).
*   [ ] **Replication expectations** documented (unlogged not present on standbys).
*   [ ] Rebuild scripts tested (re‑ingest, replay, or recompute).
*   [ ] Bulk‑load path optimized (`COPY`, `synchronous_commit=off`, delayed indexes).
*   [ ] `ANALYZE` run after bulk loads; autovacuum tuned for write patterns.
*   [ ] Partitioning plan (hot partitions unlogged; historical logged).
*   [ ] Maintenance window for `SET LOGGED/UNLOGGED` rewrites.
*   [ ] Monitoring for table persistence (`relpersistence='u'`) and bloat.

***

## Common pitfalls to avoid

*   **Accidentally relying on unlogged data** for critical features or reporting.
*   Expecting **replicas** to have the same contents: they won’t.
*   Using **foreign keys** across logged ↔ unlogged boundaries.
*   Forgetting that **indexes are also unlogged** and will be rebuilt/empty after a crash.
*   Not running `ANALYZE` after large loads → poor query plans.

***

## Quick reference: Commands

```sql
-- Create
CREATE UNLOGGED TABLE scratch (...);

-- Check persistence
SELECT relname, relpersistence
FROM pg_class
WHERE relname = 'scratch';
-- 'u' means UNLOGGED

-- Toggle
ALTER TABLE scratch SET LOGGED;   -- durable, writes WAL
ALTER TABLE scratch SET UNLOGGED; -- ephemeral, no WAL

-- Load fast
SET LOCAL synchronous_commit = OFF;
COPY scratch FROM '/path/file.csv' WITH (FORMAT csv, HEADER true);

-- Maintain stats
ANALYZE scratch;

-- Partitioning example
CREATE UNLOGGED TABLE scratch_p (...) PARTITION BY HASH (key);
CREATE UNLOGGED TABLE scratch_p_0 PARTITION OF scratch_p FOR VALUES WITH (MODULUS 4, REMAINDER 0);
```

***

## Want me to tailor this to your system?

If you can share a bit about your workload (e.g., ingest rate, replication setup, whether you use logical/physical replication, and whether the data can be reloaded from a source like Kafka or S3), I can propose a **concrete schema pattern** and a **promotion schedule** (e.g., “latest N days unlogged, older logged”), plus specific **autovacuum** and **fillfactor** settings tuned to your write rates.
