Absolutely—here’s a practical, battle‑tested guide to using PostgreSQL **temporary tables** effectively, with patterns, pitfalls, and snippets you can drop into your workflow.

***

## What a temp table is (quick refresher)

*   **Session‑local**: `TEMP`/`TEMPORARY` tables live only for your session (or transaction if you use `ON COMMIT DROP`).
*   **Schema**: They’re created under the session’s `pg_temp` schema (not visible to other sessions).
*   **Performance**: Great for complex ETL steps, de-duplication, breaking apart big joins, or caching intermediate results.
*   **Isolation**: Safe in concurrent environments—each session gets its own copy.

***

## Core best practices

### 1) Prefer `CREATE TEMP TABLE … AS` (CTAS) for speed

```sql
-- Fast load from a SELECT
CREATE TEMP TABLE tmp_orders AS
SELECT o.*
FROM orders o
WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days';

-- If you need constraints, add them after CTAS:
ALTER TABLE tmp_orders ADD PRIMARY KEY (order_id);
CREATE INDEX ON tmp_orders (customer_id);
ANALYZE tmp_orders;
```

*   CTAS avoids row‑by‑row INSERT overhead.
*   Add indexes **after** loading to reduce write amplification.

### 2) Scope explicitly with `ON COMMIT`

```sql
-- Drops at transaction end—great for functions or transactional ETL
CREATE TEMP TABLE tmp_work ON COMMIT DROP AS
SELECT ...;

-- Preserve across transactions in session
CREATE TEMP TABLE tmp_cache ON COMMIT PRESERVE ROWS AS
SELECT ...;
```

*   Use `ON COMMIT DROP` when you want guaranteed cleanup at the end of a transaction.
*   Use `ON COMMIT PRESERVE ROWS` for multi‑step interactive sessions.

### 3) Name clearly and avoid collisions

*   Use a prefix like `tmp_`, `stg_`, or `work_`.
*   If creating many in the same session, add logical suffixes: `tmp_users_stage1`, `tmp_users_dedup`.

### 4) Index only what you need

*   Create indexes on columns used in **joins, WHERE, GROUP BY, DISTINCT** of *downstream* steps.
*   Example:

```sql
CREATE INDEX tmp_orders_cust_idx ON tmp_orders (customer_id);
CREATE INDEX tmp_orders_date_idx ON tmp_orders (order_date);
ANALYZE tmp_orders;  -- update stats to guide planner
```

*   Don’t over‑index: each index adds write cost and memory while building.

### 5) Analyze after bulk load

*   `ANALYZE` temp tables to give the planner fresh stats (especially after CTAS).
*   If the table is tiny (< few thousand rows), the planner often does fine without—but it’s cheap to run.

### 6) Use `UNLOGGED` tables when you need persistence in session but faster writes

*   `UNLOGGED` tables are **not temporary**, but they skip WAL (faster writes, lost on crash).

```sql
CREATE UNLOGGED TABLE work_buffer AS
SELECT ...;
```

*   Good for staging in long ETL jobs when you need to reuse across transactions within the same DB but don’t need crash safety.
*   Not visible only to your session—be mindful of concurrency.

### 7) Prefer CTEs or subqueries for simple one‑off steps

If your “temp” is just a single intermediate result used once:

```sql
WITH recent_orders AS (
  SELECT * FROM orders WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'
)
SELECT ...
FROM recent_orders r
JOIN customers c ON c.id = r.customer_id;
```

*   CTEs keep things inline; temp tables shine when **materializing** heavy results to reuse in multiple queries or to add **indexes**.

### 8) Clean up explicitly in scripts

Even though temp tables drop on session end, be explicit in long‑running tools:

```sql
DROP TABLE IF EXISTS tmp_orders;
```

*   Helps avoid stale names if your script reconnects or runs in pooled sessions.

### 9) Avoid temp tables inside long transactions unless intended

*   Long transactions hold locks and affect vacuum. If you need many steps, consider breaking into smaller transactions and using `ON COMMIT PRESERVE ROWS` or `UNLOGGED` staging.

### 10) Watch memory & temp file usage

*   Big sorts/joins on temp tables can spill to disk. Keep an eye on:
    *   `work_mem` (per sort/hash) – tune cautiously for batch jobs.
    *   `temp_file_limit` – prevent runaway disk usage.
*   For hashing/aggregation heavy workloads, increasing `work_mem` for the session can help:

```sql
SET LOCAL work_mem = '256MB';  -- inside a transaction or session for the heavy step
```

***

## Common patterns

### Pattern A: Staging + indexing + downstream joins

```sql
BEGIN;

-- Step 1: Stage recent order lines
CREATE TEMP TABLE tmp_recent_lines AS
SELECT ol.*
FROM order_lines ol
JOIN orders o ON o.id = ol.order_id
WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days';

-- Step 2: Index on join keys and filters
CREATE INDEX ON tmp_recent_lines (product_id);
CREATE INDEX ON tmp_recent_lines (order_id);
ANALYZE tmp_recent_lines;

-- Step 3: Use staged data in a complex aggregation
SELECT p.category_id, SUM(ol.quantity) AS qty_30d
FROM tmp_recent_lines ol
JOIN products p ON p.id = ol.product_id
GROUP BY p.category_id;

COMMIT;
```

### Pattern B: Deduplication pipeline

```sql
CREATE TEMP TABLE tmp_users_raw AS
SELECT *
FROM incoming_users;  -- e.g., a loaded staging source

-- Normalize and dedup
CREATE TEMP TABLE tmp_users_clean AS
SELECT DISTINCT ON (email_lower)
  id,
  LOWER(email) AS email_lower,
  TRIM(name) AS name,
  created_at
FROM tmp_users_raw
ORDER BY email_lower, created_at DESC;

CREATE INDEX ON tmp_users_clean (email_lower);
ANALYZE tmp_users_clean;

-- Upsert into target
INSERT INTO users AS u (email, name, created_at)
SELECT email_lower, name, created_at
FROM tmp_users_clean
ON CONFLICT (email) DO UPDATE
SET name = EXCLUDED.name,
    created_at = GREATEST(u.created_at, EXCLUDED.created_at);
```

### Pattern C: Breaking apart a heavy multi‑join

Materialize parts to control join order and reuse:

```sql
-- Pre-filter big fact table
CREATE TEMP TABLE tmp_fact AS
SELECT *
FROM fact_sales
WHERE sale_date BETWEEN DATE '2025-11-01' AND DATE '2025-11-30';

CREATE INDEX ON tmp_fact (customer_id);
CREATE INDEX ON tmp_fact (product_id);
ANALYZE tmp_fact;

-- Join to smaller dims
SELECT f.*, c.region, p.category
FROM tmp_fact f
JOIN dim_customers c ON c.id = f.customer_id
JOIN dim_products  p ON p.id = f.product_id;
```

***

## Transaction and concurrency tips

*   Temp tables are **private**: you can safely use the same name across sessions.
*   Locks: temp tables still acquire locks, but only visible in your session.
*   Functions: inside `plpgsql`, `ON COMMIT DROP` helps ensure cleanup.

```sql
DO $$
DECLARE
BEGIN
  CREATE TEMP TABLE tmp_calc ON COMMIT DROP AS
  SELECT ...
  FROM ...

  -- process
  UPDATE tmp_calc SET ...;

  -- use it
  INSERT INTO final_table (...)
  SELECT ...
  FROM tmp_calc;
END $$;
```

***

## When NOT to use temp tables

*   **Single-use, simple intermediate** → use a CTE/subquery.
*   **Need sharing across sessions** → use `UNLOGGED` or regular tables in a staging schema.
*   **High churn in pooled environments** → temp table creation can cause catalog bloat; consider fewer, reusable staging tables.

***

## Performance checklist (Do/Don’t)

**Do**

*   Use CTAS to load quickly.
*   Add only necessary indexes *after* load.
*   `ANALYZE` after bulk operations.
*   Tune `work_mem` locally for heavy joins/aggregations.
*   Use `ON COMMIT DROP` in transactional ETL/function blocks.
*   Consider `UNLOGGED` for fast, non‑WAL staging across steps.

**Don’t**

*   Over‑index temp tables.
*   Keep temp tables alive across long transactions without reason.
*   Forget cleanup in scripts using connection pools.
*   Assume temp tables are faster than CTEs for trivial cases.

***

## Troubleshooting & observability

*   See temp table definitions in your session:

```sql
SELECT relname, relkind
FROM pg_class
WHERE relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'pg_temp');
```

*   Check temp file usage per backend via logs:
    *   Enable `log_temp_files = 0` (or a threshold) to log spills.
*   Watch query plans:

```sql
EXPLAIN (ANALYZE, BUFFERS) SELECT ... FROM tmp_table ...;
```

***

## Advanced: parallel + temp tables

*   Parallel workers cannot write to your temp table, but they can read if created beforehand and plan allows.
*   For parallel‑friendly pipelines, materialize the large filter first, index it, then do read‑heavy downstream steps.

***

## Ready‑to‑use template

```sql
-- Session setup
BEGIN;
SET LOCAL work_mem = '256MB';

-- Stage
DROP TABLE IF EXISTS tmp_stage;
CREATE TEMP TABLE tmp_stage AS
SELECT ...
FROM source
WHERE ...;

-- Indexes for downstream operations
CREATE INDEX tmp_stage_key_idx ON tmp_stage (key_col);
CREATE INDEX tmp_stage_filter_idx ON tmp_stage (filter_col);
ANALYZE tmp_stage;

-- Processing step
CREATE TEMP TABLE tmp_proc AS
SELECT ...
FROM tmp_stage s
JOIN dim d ON d.id = s.key_col;

CREATE INDEX ON tmp_proc (join_key);
ANALYZE tmp_proc;

-- Final write
INSERT INTO target_table (...)
SELECT ...
FROM tmp_proc;

COMMIT;
```

***

## Quick guardrails for production ETL

*   Use a dedicated **staging schema** for `UNLOGGED`/regular materializations; use temp tables for per‑job private steps.
*   Wrap each logical phase in a transaction; keep temp tables `ON COMMIT DROP` where possible.
*   Log spills and measure impact; adjust `work_mem` only locally.
*   Maintain `pg_bouncer`/pooler awareness: temp tables are per session—avoid sharing steps across pooled transactions unless you pin sessions.

***

If you share a bit more about your use case (ETL vs. reporting vs. ad‑hoc analysis, approximate row counts, and Postgres version), I can tailor a concrete pattern and sizing (indexes, `work_mem`, and plan hints) for your scenario.




Great question! The **lifetime of a PostgreSQL temporary table** depends on how you create it and the session/transaction context:

***

### ✅ **Default Lifetime**

*   A `TEMP` or `TEMPORARY` table exists **only for the duration of the session** that created it.
*   When the session ends (connection closes), the temp table is automatically dropped.

***

### ✅ **Transaction Control with `ON COMMIT`**

You can control lifetime more precisely using `ON COMMIT`:

*   **`ON COMMIT DROP`**
    *   Drops the temp table **at the end of the current transaction**.
    *   Useful for short-lived intermediate steps inside a transaction or stored procedure.
    ```sql
    CREATE TEMP TABLE tmp_data ON COMMIT DROP AS
    SELECT ...;
    ```

*   **`ON COMMIT DELETE ROWS`**
    *   Keeps the table structure but clears all rows after each commit.
    *   Rarely used, but handy for iterative inserts in the same session.
    ```sql
    CREATE TEMP TABLE tmp_buffer ON COMMIT DELETE ROWS (...);
    ```

*   **`ON COMMIT PRESERVE ROWS`** (default)
    *   Keeps the table and its data across commits until the session ends.

***

### ✅ **Key Points**

*   Temp tables are **session-scoped**, not visible to other sessions.
*   They **do not survive a reconnect** (e.g., in connection pools like PgBouncer, this matters).
*   If you need persistence beyond a session, use:
    *   **UNLOGGED tables** (faster, skip WAL, survive session end but not crash)
    *   **Regular tables in a staging schema** for multi-session workflows.

***

### ✅ **Best Practice for Lifetime**

*   For **ETL or stored procedures**: use `ON COMMIT DROP` to ensure cleanup.
*   For **interactive analysis**: default (`ON COMMIT PRESERVE ROWS`) is fine.
*   For **connection pools**: avoid temp tables unless you pin sessions, because pooled connections may reuse sessions unexpectedly.

***

Do you want me to **draw a quick lifecycle diagram** showing how temp tables behave across **session vs transaction vs commit**? Or should I give you a **code template for safe cleanup in pooled environments**?
