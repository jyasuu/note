Here's a comprehensive best-practice example combining **Retrieval-Augmented Generation (RAG)** and **Knowledge-Augmented Generation (KAG)**. For this example, we'll assume KAG refers to integrating structured knowledge graphs (KGs) with LLMs. We'll build a hybrid question-answering system using both techniques.

---

### **1. System Overview**
- **RAG**: Retrieve relevant text chunks from a vector database.
- **KAG**: Extract entities from the query and link them to a knowledge graph (e.g., Wikidata) for structured context.
- **Combined Workflow**: Merge retrieved documents and KG data to generate answers.

---

### **2. Tools & Libraries**
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2`
- **Vector DB**: `Chroma` (lightweight, in-memory)
- **Knowledge Graph**: Wikidata API
- **LLM**: `Llama-3-8B` (local) or OpenAI GPT-3.5 (API)
- **NLP Pipeline**: `spaCy` (for entity extraction)
- **Orchestration**: `LangChain`

---

### **3. Step-by-Step Implementation**

#### **Step 1: Setup Environment**
```bash
pip install langchain chromadb sentence-transformers spacy wikipedia-api
python -m spacy download en_core_web_sm
```

#### **Step 2: Data Preparation (RAG)**
- **Dataset**: Use a domain-specific text corpus (e.g., Wikipedia physics articles).
- **Chunking**: Split text into 512-token chunks with overlap.
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". "]
)
chunks = text_splitter.split_documents(documents)
```

#### **Step 3: Vector Database (RAG)**
- Use Chroma with sentence-transformers embeddings:
```python
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_db = Chroma.from_documents(chunks, embeddings)
```

#### **Step 4: Knowledge Graph Integration (KAG)**
- Extract entities using spaCy and query Wikidata:
```python
import spacy
import wikipediaapi

nlp = spacy.load("en_core_web_sm")
wiki = wikipediaapi.Wikipedia("en")

def get_kg_context(query):
    doc = nlp(query)
    entities = [ent.text for ent in doc.ents if ent.label_ in ["PERSON", "ORG", "GPE"]]
    
    kg_context = []
    for entity in entities:
        page = wiki.page(entity)
        if page.exists():
            kg_context.append(page.summary[:500])  # Truncate for brevity
    return " ".join(kg_context)
```

#### **Step 5: Hybrid Query Pipeline**
```python
from langchain.chains import RetrievalQA
from langchain.llms import LlamaCpp  # Use HuggingFacePipeline for local models

# Initialize LLM
llm = LlamaCpp(model_path="llama-3-8b.Q4_K_M.gguf", temperature=0.5)

# Build pipeline
def hybrid_qa(query):
    # RAG: Retrieve documents
    rag_results = vector_db.similarity_search(query, k=3)
    rag_context = "\n".join([doc.page_content for doc in rag_results])
    
    # KAG: Fetch KG context
    kg_context = get_kg_context(query)
    
    # Combine contexts
    combined_context = f"""
    RAG Context:
    {rag_context}
    
    Knowledge Graph Context:
    {kg_context}
    """
    
    # Generate answer
    response = llm.invoke(f"Answer this query: {query}\n\nContext: {combined_context}")
    return response
```

#### **Step 6: Query Example**
```python
query = "Explain how Einstein's theory of relativity relates to black holes"
answer = hybrid_qa(query)
print(answer)
```

---

### **4. Best Practices**
1. **Chunk Optimization**: 
   - Use smaller chunks (128-512 tokens) for factoid QA, larger for summarization.
   - Experiment with overlap (10-20% of chunk size).

2. **Embedding Models**:
   - Prefer instruction-tuned embeddings (e.g., `bge-large-en-v1.5`) for retrieval.

3. **Knowledge Graph Caching**:
   - Cache frequent KG queries to avoid API latency.

4. **Validation**:
   - Use metrics like `Hit Rate` (RAG) and `Entity Linking Accuracy` (KAG).
   - Human evaluation for answer coherence.

5. **Prompt Engineering**:
   - Explicitly separate RAG/KAG contexts in the prompt.
   - Add fallback logic: "If contexts conflict, prioritize RAG."

---

### **5. Advanced Enhancements**
- **Re-Ranking**: Add a cross-encoder (e.g., `bge-reranker-large`) to improve retrieval quality.
- **KG Embeddings**: Store pre-computed KG entity embeddings for faster lookups.
- **Hybrid Search**: Combine vector + keyword search (e.g., BM25).

---

### **6. Evaluation Template**
```python
test_cases = [
    {"query": "What is CRISPR?", "expected_entities": ["CRISPR"]},
    {"query": "Explain Tesla's contributions to AC power", "expected_kg_links": ["Nikola Tesla"]}
]

def evaluate_system():
    for case in test_cases:
        kg_context = get_kg_context(case["query"])
        assert any(entity in kg_context for entity in case["expected_entities"])
```

---

This template balances simplicity with production-ready practices. Adjust based on your use case (e.g., swap Wikidata for a domain-specific KG like PubMed for medical QA).
