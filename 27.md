I'll help you create a comprehensive benchmark setup. Here's a step-by-step guide:

1. **Docker Compose Setup** (`docker-compose.yml`):
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpass
      POSTGRES_DB: testdb
    ports:
      - "5432:5432"
    volumes:
      - ./init/postgres:/docker-entrypoint-initdb.d
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U testuser -d testdb"]
      interval: 5s
      timeout: 5s
      retries: 5

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.7.1
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - ./init/elasticsearch:/usr/share/elasticsearch/init
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  pgdata:
  esdata:
```

2. **Initialization Scripts**:

For PostgreSQL (`init/postgres/init.sql`):
```sql
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_fts ON documents USING GIN (to_tsvector('english', title || ' ' || content));
```

For Elasticsearch (`init/elasticsearch/create-index.sh`):
```bash
#!/bin/bash
until curl -sX PUT "http://localhost:9200/documents" -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "title": { "type": "text", "analyzer": "english" },
      "content": { "type": "text", "analyzer": "english" },
      "created_at": { "type": "date" }
    }
  }
}'; do
  sleep 1
done
```
Make the script executable: `chmod +x init/elasticsearch/create-index.sh`

3. **Mock Data Generation** (Rust):

Add dependencies to `Cargo.toml`:
```toml
[dependencies]
faker = "0.3"
rand = "0.8"
tokio = { version = "1.0", features = ["full"] }
```

Data generator (`src/generate_data.rs`):
```rust
use faker::Faker;
use faker::locales::EN;
use rand::Rng;

pub async fn generate_documents(count: usize) -> Vec<String> {
    let mut rng = rand::thread_rng();
    let faker = Faker::new(EN);
    
    let mut docs = Vec::with_capacity(count);
    
    for _ in 0..count {
        let title = faker.fake::<&str, _>("lorem.sentence(3, 5)");
        let content = faker.fake::<&str, _>("lorem.paragraph(5, 10, 20, \"\n\")");
        let created_at = chrono::Utc::now() - chrono::Duration::days(rng.gen_range(0..365));
        
        docs.push(format!(
            r#"{{ "title": "{}", "content": "{}", "created_at": "{}" }}"#,
            title.replace('"', r#"\""#),
            content.replace('"', r#"\""#),
            created_at.to_rfc3339()
        ));
    }
    
    docs
}
```

4. **Benchmark Code** (Rust):

Add dependencies:
```toml
[dependencies]
tokio-postgres = { version = "0.7", features = ["with-uuid-1"] }
elasticsearch = { version = "8.6", features = ["rustls"] }
reqwest = { version = "0.11", features = ["json"] }
serde_json = "1.0"
csv = "1.2"
```

Benchmark structure (`src/main.rs`):
```rust
use std::time::Instant;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize connections
    let pg_client = connect_postgres().await?;
    let es_client = connect_elasticsearch().await?;

    // Generate test data (10k documents)
    let docs = generate_data::generate_documents(10_000).await;

    // Insert data into both databases
    insert_postgres(&pg_client, &docs).await?;
    insert_elasticsearch(&es_client, &docs).await?;

    // Run benchmarks
    let queries = vec![
        "database performance",
        "search engine",
        "distributed systems",
        "rust programming",
        "benchmark results"
    ];

    benchmark_postgres(&pg_client, &queries).await?;
    benchmark_elasticsearch(&es_client, &queries).await?;

    Ok(())
}

async fn benchmark_postgres(client: &tokio_postgres::Client, queries: &[&str]) -> Result<(), Box<dyn std::error::Error>> {
    let mut total_latency = 0.0;
    let mut count = 0;

    for query in queries {
        let start = Instant::now();
        let rows = client.query(
            "SELECT id, title, ts_rank_cd(to_tsvector('english', title || ' ' || content), plainto_tsquery('english', $1)) as rank 
             FROM documents 
             WHERE to_tsvector('english', title || ' ' || content) @@ plainto_tsquery('english', $1)
             ORDER BY rank DESC
             LIMIT 10",
            &[&query]
        ).await?;
        
        let duration = start.elapsed().as_secs_f64();
        total_latency += duration;
        count += 1;
    }

    println!("PostgreSQL Average Latency: {:.4}ms", (total_latency / count as f64) * 1000.0);
    Ok(())
}

async fn benchmark_elasticsearch(client: &elasticsearch::Elasticsearch, queries: &[&str]) -> Result<(), Box<dyn std::error::Error>> {
    let mut total_latency = 0.0;
    let mut count = 0;

    for query in queries {
        let start = Instant::now();
        let response = client
            .search(elasticsearch::SearchParts::Index(&["documents"]))
            .body(json!({
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": ["title", "content"]
                    }
                },
                "size": 10
            }))
            .send()
            .await?;
        
        let duration = start.elapsed().as_secs_f64();
        total_latency += duration;
        count += 1;
    }

    println!("Elasticsearch Average Latency: {:.4}ms", (total_latency / count as f64) * 1000.0);
    Ok(())
}
```

5. **Test Cases**:
- Basic full-text search
- Phrase search
- Combined filters (date range + search)
- Pagination performance
- Multi-field search

6. **Running the Benchmark**:
```bash
# Start services
docker-compose up -d

# Wait for services to be healthy
sleep 30

# Run benchmark
cargo run --release
```

7. **Expected Output**:
- Insertion speed comparison
- Query latency comparison
- Memory usage (you can add Docker stats monitoring)
- CPU utilization during queries

8. **Additional Considerations**:
- Use different dataset sizes (1k, 10k, 100k documents)
- Test with concurrent requests
- Compare indexing speeds
- Monitor resource usage during operations
- Test relevance of results

To make this production-ready, you might want to add:
- Proper error handling
- Configuration files
- CSV output for results
- Statistical analysis of results
- Resource monitoring integration
- Automated report generation

Would you like me to elaborate on any of these components or provide additional implementation details for specific parts?
